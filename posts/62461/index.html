<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><link href="https://fastly.jsdelivr.net/npm/hexo-tag-common@0.1.0/css/index.css" rel="stylesheet"><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png"><link rel="icon" href="/img/fluid.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests"><meta name="theme-color" content="#2f4154"><meta name="author" content="AWEI"><meta name="keywords" content="AWEI,AWEI的技术小屋"><meta name="description" content="网络爬虫什么是网络爬虫网络爬虫（又被称为网页蜘蛛、网络机器人），是一种按照一定的规则，自动地抓取互联网信息的程序或者脚本。 其本质是，由一个入口网页进行深入，不停的进行其他的 URL 的爬取，然后再把抓取到的网页进行分析处理得出想要的数据。 随着大数据时代的来临，网络爬虫在互联网中的地位将越来越重要。互联网中的数据是海量的，如何自动高效地获取互联网中我们感兴趣的信息并为我们所用是一个重要的问题，而"><meta property="og:type" content="article"><meta property="og:title" content="python 爬虫"><meta property="og:url" content="https://www.inencoding.com/posts/62461/index.html"><meta property="og:site_name" content="In-Encoding"><meta property="og:description" content="网络爬虫什么是网络爬虫网络爬虫（又被称为网页蜘蛛、网络机器人），是一种按照一定的规则，自动地抓取互联网信息的程序或者脚本。 其本质是，由一个入口网页进行深入，不停的进行其他的 URL 的爬取，然后再把抓取到的网页进行分析处理得出想要的数据。 随着大数据时代的来临，网络爬虫在互联网中的地位将越来越重要。互联网中的数据是海量的，如何自动高效地获取互联网中我们感兴趣的信息并为我们所用是一个重要的问题，而"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.inencoding.com/img/%E6%96%87%E7%AB%A0/reptile.jpg"><meta property="article:published_time" content="2022-08-13T15:08:18.000Z"><meta property="article:modified_time" content="2022-12-08T15:28:22.393Z"><meta property="article:author" content="AWEI"><meta property="article:tag" content="爬虫"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://www.inencoding.com/img/%E6%96%87%E7%AB%A0/reptile.jpg"><meta name="referrer" content="no-referrer-when-downgrade"><title>python 爬虫 - In-Encoding</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="/css/fluid-extention.css"><script id="fluid-configs">var dntVal,Fluid=window.Fluid||{},CONFIG=(Fluid.ctx=Object.assign({},Fluid.ctx),{hostname:"www.inencoding.com",root:"/",version:"1.9.2",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,follow_dnt:!1,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1}},search_path:"https://cdn.jsdelivr.net/gh/AWeiIsCoding/AWeiIsCoding.github.io/local-search.xml"});CONFIG.web_analytics.follow_dnt&&(dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on")))</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><style>.spoiler{display:inline-flex}p.spoiler{display:flex}.spoiler a{pointer-events:none}.spoiler-blur,.spoiler-blur>*{transition:text-shadow .5s ease}.spoiler .spoiler-blur,.spoiler .spoiler-blur>*{color:transparent;background-color:rgba(0,0,0,0);text-shadow:0 0 10px grey;cursor:pointer}.spoiler .spoiler-blur:hover,.spoiler .spoiler-blur:hover>*{text-shadow:0 0 5px grey}.spoiler-box,.spoiler-box>*{transition:color .5s ease,background-color .5s ease}.spoiler .spoiler-box,.spoiler .spoiler-box>*{color:#000;background-color:#000;text-shadow:none}</style><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="In-Encoding" type="application/atom+xml"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>InEncoding</strong></a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/background.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="python 爬虫"></span></div><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> AWEI</span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2022-08-13 23:08" pubdate>2022年8月13日 晚上</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 50k 字</span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 416 分钟</span><span id="busuanzi_container_page_pv" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span> 次</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar category-bar" style="margin-right:-1rem"><div class="category-list"><div class="category row nomargin-x"><a class="category-item list-group-item category-item-action col-10 col-md-11 col-xm-11" title="学习笔记" id="heading-078425eaf316a180b0989442e53f920b" role="tab" data-toggle="collapse" href="#collapse-078425eaf316a180b0989442e53f920b" aria-expanded="true">学习笔记 <span class="list-group-count">(23)</span><i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse show" id="collapse-078425eaf316a180b0989442e53f920b" role="tabpanel" aria-labelledby="heading-078425eaf316a180b0989442e53f920b"><div class="category-post-list"><a href="/posts/53027/" title="Ajax 和 Json" class="list-group-item list-group-item-action"><span class="category-post">Ajax 和 Json</span></a> <a href="/posts/40991/" title="Docker" class="list-group-item list-group-item-action"><span class="category-post">Docker</span></a> <a href="/posts/14402/" title="Docker 常用环境搭建" class="list-group-item list-group-item-action"><span class="category-post">Docker 常用环境搭建</span></a> <a href="/posts/22111/" title="JQuery 学习笔记" class="list-group-item list-group-item-action"><span class="category-post">JQuery 学习笔记</span></a> <a href="/posts/43258/" title="Java 多线程学习笔记" class="list-group-item list-group-item-action"><span class="category-post">Java 多线程学习笔记</span></a> <a href="/posts/12475/" title="Java 注解与反射" class="list-group-item list-group-item-action"><span class="category-post">Java 注解与反射</span></a> <a href="/posts/15254/" title="Java 网络编程学习笔记" class="list-group-item list-group-item-action"><span class="category-post">Java 网络编程学习笔记</span></a> <a href="/posts/16781/" title="JavaWeb 学习笔记" class="list-group-item list-group-item-action"><span class="category-post">JavaWeb 学习笔记</span></a> <a href="/posts/33152/" title="MyBatis-Plus" class="list-group-item list-group-item-action"><span class="category-post">MyBatis-Plus</span></a> <a href="/posts/39805/" title="MySQL 学习笔记" class="list-group-item list-group-item-action"><span class="category-post">MySQL 学习笔记</span></a> <a href="/posts/15608/" title="Mybatis" class="list-group-item list-group-item-action"><span class="category-post">Mybatis</span></a> <a href="/posts/27273/" title="Redis" class="list-group-item list-group-item-action"><span class="category-post">Redis</span></a> <a href="/posts/18155/" title="Spring" class="list-group-item list-group-item-action"><span class="category-post">Spring</span></a> <a href="/posts/46897/" title="Spring MVC" class="list-group-item list-group-item-action"><span class="category-post">Spring MVC</span></a> <a href="/posts/33757/" title="SpringBoot" class="list-group-item list-group-item-action"><span class="category-post">SpringBoot</span></a> <a href="/posts/42622/" title="SpringCloud" class="list-group-item list-group-item-action"><span class="category-post">SpringCloud</span></a> <a href="/posts/43782/" title="Vue" class="list-group-item list-group-item-action"><span class="category-post">Vue</span></a> <a href="/posts/43167/" title="git" class="list-group-item list-group-item-action"><span class="category-post">git</span></a> <a href="/posts/62461/" title="python 爬虫" class="list-group-item list-group-item-action active"><span class="category-post">python 爬虫</span></a> <a href="/posts/13607/" title="基于 Centos6.5 的 Httpd 安装" class="list-group-item list-group-item-action"><span class="category-post">基于 Centos6.5 的 Httpd 安装</span></a> <a href="/posts/6262/" title="基于 Centos6.8 的 Hadoop 分布式集群安装" class="list-group-item list-group-item-action"><span class="category-post">基于 Centos6.8 的 Hadoop 分布式集群安装</span></a> <a href="/posts/59309/" title="基于 Centos7 的 Zookeeper 安装" class="list-group-item list-group-item-action"><span class="category-post">基于 Centos7 的 Zookeeper 安装</span></a> <a href="/posts/13858/" title="软件工程随笔" class="list-group-item list-group-item-action"><span class="category-post">软件工程随笔</span></a></div></div></div></div></aside></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 style="display:none">python 爬虫</h1><p class="note note-warning">本文最后更新于：2022年12月8日 晚上</p><div class="markdown-body"><h1 id="网络爬虫"><a href="#网络爬虫" class="headerlink" title="网络爬虫"></a>网络爬虫</h1><h2 id="什么是网络爬虫"><a href="#什么是网络爬虫" class="headerlink" title="什么是网络爬虫"></a>什么是网络爬虫</h2><p><strong>网络爬虫</strong>（又被称为网页蜘蛛、网络机器人），是一种按照一定的规则，自动地抓取互联网信息的程序或者脚本。</p><p><strong>其本质是，由一个入口网页进行深入，不停的进行其他的 URL 的爬取，然后再把抓取到的网页进行分析处理得出想要的数据。</strong></p><p>随着大数据时代的来临，网络爬虫在互联网中的地位将越来越重要。互联网中的数据是海量的，如何自动高效地获取互联网中我们感兴趣的信息并为我们所用是一个重要的问题，而爬虫技术就是为了解决这些问题而生的。</p><h2 id="爬虫的核心"><a href="#爬虫的核心" class="headerlink" title="爬虫的核心"></a>爬虫的核心</h2><ol><li><p>爬取网页：爬取整个网页，包含了网页中所有得内容。</p></li><li><p>解析数据：将网页中你得到的数据，进行解析。</p></li><li><p>难点：爬虫和反爬虫之间的博弈。</p></li></ol><h2 id="网络爬虫的用途"><a href="#网络爬虫的用途" class="headerlink" title="网络爬虫的用途"></a>网络爬虫的用途</h2><ul><li>数据分析 / 人工数据集</li><li>社交软件冷启动</li><li>舆情监控</li><li>竞争对手监控</li><li>。。。</li></ul><h2 id="爬虫分类"><a href="#爬虫分类" class="headerlink" title="爬虫分类"></a>爬虫分类</h2><ul><li><strong>通用爬虫</strong><ul><li><strong>实例</strong>：Google、百度、搜狗等搜索引擎</li><li><strong>功能</strong>：访问网页‐&gt; 抓取数据‐&gt; 数据存储‐&gt; 数据处理‐&gt; 提供检索服务</li><li><strong>Robots 协议</strong>：是网站跟爬虫间的协议，用简单直接的 txt 格式文本方式告诉对应的爬虫被允许的权限。</li><li><strong>网站排名</strong>（SEO）：根据 pagerank 算法值进行排名（参考个网站流量、点击率等指标）</li></ul></li><li><strong>聚焦爬虫</strong><ul><li><strong>功能</strong>：根据需求，实现爬虫程序，抓取需要的数据</li><li><strong>设计思路</strong>：<ol><li>确定要爬取的 url</li><li>模拟浏览器通过 http 协议访问 url，获取服务器返回的 html 代码</li><li>解析 html 字符串（根据一定规则提取需要的数据）</li></ol></li></ul></li></ul><h2 id="反反爬手段"><a href="#反反爬手段" class="headerlink" title="反反爬手段"></a>反反爬手段</h2><ul><li><strong>User‐Agent</strong>：用户代理，简称 UA，它是一个特殊字符串头，使得服务器能够识别客户使用的操作系统及版本、CPU 类型、浏览器及版本、浏览器渲染引擎、浏览器语言、浏览器插件等。</li><li><strong>使用代理 IP</strong>：<ul><li><a target="_blank" rel="noopener" href="https://free.kuaidaili.com/free/">快代理</a></li><li><a target="_blank" rel="noopener" href="https://proxy.ip3366.net/free/">齐云代理</a></li><li><a target="_blank" rel="noopener" href="https://github.com/jhao104/proxy_pool">Proxy_Pool 爬虫代理 IP 池①</a></li><li><a target="_blank" rel="noopener" href="https://github.com/Python3WebSpider/ProxyPool">Proxy_Pool 爬虫代理 IP 池②</a></li><li>什么是高匿名、匿名和透明代理？它们有什么区别？<ul><li>使用透明代理，对方服务器可以知道你使用了代理，并且也知道你的真实 IP。</li><li>使用匿名代理，对方服务器可以知道你使用了代理，但不知道你的真实 IP。</li><li>使用高匿名代理，对方服务器不知道你使用了代理，更不知道你的真实 IP。</li></ul></li></ul></li><li><strong>验证码</strong>：<ul><li>打码平台</li><li>云打码平台</li><li>超级鹰</li></ul></li><li><strong>分析动态加载网页</strong>：网站返回的是 js 数据 并不是网页的真实数据<ul><li>selenium 驱动真实的浏览器发送请求</li></ul></li><li><strong>数据解密</strong>：<ul><li>分析 js 代码</li></ul></li></ul><h1 id="urllib"><a href="#urllib" class="headerlink" title="urllib"></a>urllib</h1><h2 id="urllib-库简介"><a href="#urllib-库简介" class="headerlink" title="urllib 库简介"></a>urllib 库简介</h2><p>urllib 是 Python 标准库中最常用的 Python 网页访问的模块，它可以让用户像访问本地文本文件一样读取网页的内容。 Python2 系列使用的是 urllib2, <strong>Python3 以后将其全部整合为 urllib; 在 Python3.x 中，用户可以使用 urllib 这个库抓取网页</strong>。<br><strong>urllib 库提供了一个网页访问的简单易懂的 API 接口，还包括一些函数方法，用于进行参数编码、下载网页等</strong>操作。这个模块的使用门槛非常低，初学者也可以尝试去抓取和读取或者保存网页。 urllib 是ー个 URL 处理包，在这个包中集合了一些处理 URL 的模块。</p><ol><li><code>urllib.request</code> 模块：用来打开和读取 URL</li><li><code>urllib.error</code> 模块：包含一些由 <code>urllib.request</code> 产生的错误，可以使用 try 进行捕捉处理</li><li><code>urllib.parse</code> 模块：包含一些解析 URL 的方法</li><li><code>urllib.robotparser</code> 模块：用来解析 robots.txt 文本文件。它提供了一个单独的 <code>RobotFileparser</code> 类，通过该类提供的 <code>can_fetch()</code> 方法测试爬虫是否可以下载一个页面</li></ol><h2 id="urllib-库的使用"><a href="#urllib-库的使用" class="headerlink" title="urllib 库的使用"></a>urllib 库的使用</h2><ul><li><code>urllib.request.urlopen ()</code> 模拟浏览器向服务器发送请求</li><li>response 服务器返回的数据：<strong>一个类型，六个方法</strong><ul><li>response 的数据类型是 HttpResponse</li><li>字节‐‐&gt; 字符串 解码 decode</li><li>字符串‐‐&gt; 字节 编码 encode</li><li><code>read ()</code> 字节形式读取二进制 扩展：<strong>rede (n) 返回前 n 个字节</strong></li><li><code>readline ()</code> 只能 <strong>读取一行</strong></li><li><code>readlines ()</code> 一行一行读取 <strong>直至结束</strong></li><li><code>getcode ()</code> 获取状态码 （判断书写逻辑是否正确，返回 200 表示正常）</li><li><code>geturl ()</code> 获取 url 地址</li><li><code>getheaders ()</code> 获取 headers</li></ul></li></ul><h3 id="案例：爬取百度首页的HTML代码"><a href="#案例：爬取百度首页的HTML代码" class="headerlink" title="案例：爬取百度首页的HTML代码"></a>案例：爬取百度首页的 HTML 代码</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> urllib.request<br><br><span class="hljs-comment"># 使用urllib来获取百度首页源码</span><br><span class="hljs-comment"># 1.定义一个url 你要访问的地址</span><br><br>url = <span class="hljs-string">'http://www.baidu.com'</span><br><br><span class="hljs-comment"># 2.模拟浏览器向服务器发送请求</span><br>response = urllib.request.urlopen(url)<br><br><span class="hljs-comment"># 3.获取响应中的页面源码</span><br><span class="hljs-comment"># read方法 返回的是字节形式的二进制数据</span><br><span class="hljs-comment"># 将二进制的数据转换成字符串</span><br>content = response.read().decode(<span class="hljs-string">'utf-8'</span>)<br><br><span class="hljs-comment"># 4.打印数据</span><br><span class="hljs-built_in">print</span>(content)<br></code></pre></td></tr></tbody></table></figure><p>运行结果：</p><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220814232957.png" srcset="/img/loading.gif" lazyload alt="image-20220814232957184"></p><h2 id="urllib下载"><a href="#urllib下载" class="headerlink" title="urllib下载"></a>urllib 下载</h2><p><code>urlretrieve (url,filename)</code> 获取资源，下载到本地</p><h3 id="案例：下载图片视频"><a href="#案例：下载图片视频" class="headerlink" title="案例：下载图片视频"></a>案例：下载图片视频</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os.path<br><span class="hljs-keyword">import</span> urllib.request<br><br><span class="hljs-comment"># 下载网页</span><br>url_page = <span class="hljs-string">"http://www.baidu.com"</span><br><br><span class="hljs-comment"># 下载图片</span><br>url_img = <span class="hljs-string">'https://ts1.cn.mm.bing.net/th/id/R-C.9d2f94190cd332dcc58a406b86a9632d?rik=%2fwzDTXG%2byszt3Q&amp;riu=http%3a%2f%2fimg.zcool.cn%2fcommunity%2f01801d5619d59d32f8755701932582.jpg&amp;ehk=OiCsHvxm9qzPhgwWLmNL2luZVvW3SUvUDNLG5dz%2ffb4%3d&amp;risl=&amp;pid=ImgRaw&amp;r=0'</span><br><br><span class="hljs-comment"># 下载视频</span><br>url_video = <span class="hljs-string">'https://apd-988280ccc495f807042d5d01af540fad.v.smtcdns.com/om.tc.qq.com/AGmstw0bclgHrcC2s4HMMERnVqmqkuznWSNJCFiSudxo/uwMROfz2r57CIaQXGdGnC2ddDmYEwcIXuKj5NQsSsUwCcArN/svp_50001/shg_56839604_50001_195d9bd530d4329e9b6003d890ffaa7b.f622.mp4?sdtfrom=v1010&amp;guid=fda519cd84f8daf0&amp;vkey=1BA564CCBA63DB71E2C8545EB3C58C2E8ABD0DBDB1268D8444A930E0DA3A6DA861E6F962A857D88D3ADD33F7611FC08215B1E6BB0DC307F90B5F415B546127D308ACA21428F933BD7262113B77E8770D58B3257D9A28CCFC2F50E5C07ED354558E41A126C0B8757A8984E2286733EA33758AE41B3F7B54F5FBCFF806C7B69F0C'</span><br><br><span class="hljs-comment"># 创建资源文件夹</span><br>filepath = <span class="hljs-string">'./download'</span><br><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(filepath):<br>    os.makedirs(filepath)<br><br><span class="hljs-comment"># 下载到资源文件夹</span><br>urllib.request.urlretrieve(url_page, <span class="hljs-string">"./download/百度.html"</span>)<br>urllib.request.urlretrieve(url_img, <span class="hljs-string">"./download/陈奕迅.jpg"</span>)<br>urllib.request.urlretrieve(url_video, <span class="hljs-string">"./download/最佳损友.mp4"</span>)<br></code></pre></td></tr></tbody></table></figure><p>运行结果：</p><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220815122142.png" srcset="/img/loading.gif" lazyload alt="image-20220815122142584"></p><h2 id="请求对象的定制应对UA反爬"><a href="#请求对象的定制应对UA反爬" class="headerlink" title="请求对象的定制应对UA反爬"></a>请求对象的定制应对 UA 反爬</h2><p><strong>UA 介绍</strong>：User Agent 中文名为用户代理，简称 UA，它是一个特殊字符串头，使得服务器能够识别客户使用的操作系统 及版本、CPU 类型、浏览器及版本。浏览器内核、浏览器渲染引擎、浏览器语言、浏览器插件等</p><p><strong>语法：</strong><code>request = urllib.request.Request ()</code></p><h3 id="url-的组成"><a href="#url-的组成" class="headerlink" title="url 的组成"></a>url 的组成</h3><p><code>https://www.baidu.com/s?wd=周杰伦</code></p><ul><li><code>http / https</code>：协议</li><li><code>www.baidu.com</code>：主机</li><li><code>80 / 443</code>：端口</li><li><code>wd = 周杰伦</code>：参数</li><li><code>s</code> ：路径</li><li><code>#</code>：锚点</li></ul><h3 id="案例：UA定制应对反爬"><a href="#案例：UA定制应对反爬" class="headerlink" title="案例：UA定制应对反爬"></a>案例：UA 定制应对反爬</h3><p>当爬取 <code>https</code> 协议的网页时，有可能会遭遇 UA 反爬，所以需要定制 UA，欺骗服务器。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> urllib.request<br><br>url = <span class="hljs-string">'https://www.baidu.com'</span><br><br>headers = {<br>    <span class="hljs-string">'User-Agent'</span>: <span class="hljs-string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'</span><br>}<br><br><span class="hljs-comment"># 因为urlopen方法中不能存储字典 所以headers不能传递进去</span><br><span class="hljs-comment"># 请求对象的定制</span><br><span class="hljs-comment"># 之所以写url=url,headers=headers,而不直接写url,headers是因为源码def __init__(self, url, data=None, headers={},origin_req_host=None, unverifiable=False,method=None)的第三参数才是headers,如果直接写，那么会把headers当作第二个参数</span><br>request = urllib.request.Request(url=url, headers=headers)<br><br>response = urllib.request.urlopen(request)<br><br><span class="hljs-built_in">print</span>(response.read().decode(<span class="hljs-string">'utf8'</span>))<br></code></pre></td></tr></tbody></table></figure><p>未定制 UA 运行结果：</p><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220815163541.png" srcset="/img/loading.gif" lazyload alt="image-20220815163541008"></p><p>定制 UA 运行结果：</p><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220815163450.png" srcset="/img/loading.gif" lazyload alt="image-20220815163450758"></p><h2 id="编码集"><a href="#编码集" class="headerlink" title="编码集"></a>编码集</h2><p>由于计算机是美国人发明的，因此，最早只有 127 个字符被编码到计算机里，也就是大小写英文字母、数字和一些符号， 这个编码表被称为 ASCII 编码，比如大写字母 A 的编码是 65，小写字母 z 的编码是 122。 <strong>但是要处理中文显然一个字节是不够的，至少需要两个字节，而且还不能和 ASCII 编码冲突， 所以，中国制定了 GB2312 编码，用来把中文编进去。 你可以想得到的是，全世界有上百种语言，日本把日文编到 Shift_JIS 里，韩国把韩文编到 Euc‐kr 里</strong>，各国有各国的标准，就会不可避免地出现冲突，结果就是，在多语言混合的文本中，显示出来会有乱码。 因此，Unicode 应运而生。Unicode 把所有语言都统一到一套编码里，这样就不会再有乱码问题了。 Unicode 标准也在不断发展，但最常用的是用两个字节表示一个字符（如果要用到非常偏僻的字符，就需要 4 个字节）。 现代操作系统和大多数编程语言都直接支持 Unicode。</p><p><code>urllib.parse.urlencode ()</code><strong>就是将中文变成 Unicode 编码，这样才不会报错</strong></p><h3 id="get-请求方式：quote方法"><a href="#get-请求方式：quote方法" class="headerlink" title="get 请求方式：quote方法"></a>get 请求方式：quote 方法</h3><p>quote（）方法能够将汉字转换成 unicode 编码的格式，适用于<strong>单个参数</strong></p><h4 id="案例：搜索单参数"><a href="#案例：搜索单参数" class="headerlink" title="案例：搜索单参数"></a>案例：搜索单参数</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> urllib.parse<br><span class="hljs-keyword">import</span> urllib.request<br><br>url = <span class="hljs-string">'https://www.baidu.com/s?wd='</span><br><br><span class="hljs-comment"># 请求对象的定制为了解决反爬的第一种手段</span><br>headers = {<br>    <span class="hljs-string">'Cookie'</span>: <span class="hljs-string">'xxx'</span>,<br>    <span class="hljs-string">'User-Agent'</span>: <span class="hljs-string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'</span><br>}<br><br><span class="hljs-comment"># 将周杰伦三个字变成unicode编码的格式</span><br><span class="hljs-comment"># 我们需要依赖于urllib.parse</span><br>name = urllib.parse.quote(<span class="hljs-string">'周杰伦'</span>)<br><br>url = url + name<br><br><span class="hljs-comment"># 请求对象的定制</span><br>request = urllib.request.Request(url=url, headers=headers)<br><br><span class="hljs-comment"># 模拟浏览器向服务器发送请求</span><br>response = urllib.request.urlopen(request)<br><br><span class="hljs-comment"># 获取响应的内容</span><br>content = response.read().decode(<span class="hljs-string">'utf-8'</span>)<br><br><span class="hljs-comment"># 打印数据</span><br><span class="hljs-built_in">print</span>(content)<br></code></pre></td></tr></tbody></table></figure><p>运行结果：</p><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220815194608.png" srcset="/img/loading.gif" lazyload alt="image-20220815194608826"></p><h3 id="get-请求方式：urlencode方法"><a href="#get-请求方式：urlencode方法" class="headerlink" title="get 请求方式：urlencode方法"></a>get 请求方式：urlencode 方法</h3><p><code>urlencode()</code> 方法也可以将汉字转换成 unicode 编码，适用于<strong>多个参数</strong></p><h4 id="案例：搜索多参数"><a href="#案例：搜索多参数" class="headerlink" title="案例：搜索多参数"></a>案例：搜索多参数</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># urlencode应用场景：多个参数的时候</span><br><span class="hljs-keyword">import</span> urllib.parse<br><span class="hljs-keyword">import</span> urllib.request<br><br>base_url = <span class="hljs-string">'https://www.baidu.com/s?'</span><br><br>data = {<br>    <span class="hljs-string">'wd'</span>: <span class="hljs-string">'周杰伦'</span>,<br>    <span class="hljs-string">'sex'</span>: <span class="hljs-string">'男'</span>,<br>    <span class="hljs-string">'location'</span>: <span class="hljs-string">'中国台湾省'</span><br>}<br><br>new_data = urllib.parse.urlencode(data)<br><br><span class="hljs-comment"># 请求资源路径</span><br>url = base_url + new_data<br><br><span class="hljs-comment"># 请求对象的定制为了解决反爬的第一种手段</span><br>headers = {<br>    <span class="hljs-string">'Cookie'</span>: <span class="hljs-string">'xxx'</span>,<br>    <span class="hljs-string">'User-Agent'</span>: <span class="hljs-string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'</span><br>}<br><br><span class="hljs-comment"># 请求对象的定制</span><br>request = urllib.request.Request(url=url, headers=headers)<br><br><span class="hljs-comment"># 模拟浏览器向服务器发送请求</span><br>response = urllib.request.urlopen(request)<br><br><span class="hljs-comment"># 获取网页源码的数据</span><br>content = response.read().decode(<span class="hljs-string">'utf-8'</span>)<br><br><span class="hljs-comment"># 打印数据</span><br><span class="hljs-built_in">print</span>(content)<br></code></pre></td></tr></tbody></table></figure><p>运行结果：</p><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220815194951.png" srcset="/img/loading.gif" lazyload alt="image-20220815194951387"></p><h3 id="post-请求方式"><a href="#post-请求方式" class="headerlink" title="post 请求方式"></a>post 请求方式</h3><p>post 请求方式与 get 请求方式异同：</p><ul><li>相同点<ul><li>get 请求方式和 post 请求方式的<strong>参数都需要必须编码</strong></li></ul></li><li>不同点<ul><li>get 请求方式的<strong>参数是拼接到 url 后面</strong>，编码之后<strong>不需要调用</strong> <code>encode</code> 方法</li><li>post 请求方式的<strong>参数是放在请求对象定制的方法中</strong>，编码之后<strong>需要调用</strong> <code>encode</code> 方法</li></ul></li></ul><h4 id="案例：百度翻译"><a href="#案例：百度翻译" class="headerlink" title="案例：百度翻译"></a>案例：百度翻译</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># post请求</span><br><br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> urllib.parse<br><span class="hljs-keyword">import</span> urllib.request<br><br>url = <span class="hljs-string">'https://fanyi.baidu.com/sug'</span><br><br>headers = {<br>    <span class="hljs-string">'User-Agent'</span>: <span class="hljs-string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'</span><br>}<br><br>data = {<br>    <span class="hljs-string">'kw'</span>: <span class="hljs-string">'reptile'</span><br>}<br><br><span class="hljs-comment"># post请求的参数 必须要进行编码</span><br>data = urllib.parse.urlencode(data).encode(<span class="hljs-string">'utf-8'</span>)<br><br><span class="hljs-comment"># post的请求的参数 是不会拼接在url的后面的  而是需要放在请求对象定制的参数中</span><br><span class="hljs-comment"># post请求的参数 必须要进行编码</span><br>request = urllib.request.Request(url=url, data=data, headers=headers)<br><br><span class="hljs-comment"># 模拟浏览器向服务器发送请求</span><br>response = urllib.request.urlopen(request)<br><br><span class="hljs-comment"># 获取响应的数据</span><br>content = response.read().decode(<span class="hljs-string">'utf-8'</span>)<br><br><span class="hljs-comment"># 字符串--》json对象</span><br>obj = json.loads(content)<br><span class="hljs-built_in">print</span>(obj)<br></code></pre></td></tr></tbody></table></figure><p>运行结果：</p><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220815200515.png" srcset="/img/loading.gif" lazyload alt="image-20220815200515615"></p><h2 id="爬取ajax请求"><a href="#爬取ajax请求" class="headerlink" title="爬取ajax请求"></a>爬取 ajax 请求</h2><h3 id="ajax的get请求"><a href="#ajax的get请求" class="headerlink" title="ajax的get请求"></a>ajax 的 get 请求</h3><h4 id="案例：爬取一页豆瓣电影"><a href="#案例：爬取一页豆瓣电影" class="headerlink" title="案例：爬取一页豆瓣电影"></a>案例：爬取一页豆瓣电影</h4><p>爬取<a target="_blank" rel="noopener" href="https://movie.douban.com/typerank?type_name=%E6%82%AC%E7%96%91&amp;type=10&amp;interval_id=100:90&amp;action=">豆瓣电影排行榜中悬疑类电影</a>的第一页</p><p>首先找到 url 请求地址</p><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220815203500.png" srcset="/img/loading.gif" lazyload alt="image-20220815203500325"></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> urllib.request<br><br>url = <span class="hljs-string">'https://movie.douban.com/j/chart/top_list?type=10&amp;interval_id=100%3A90&amp;action=&amp;start=0&amp;limit=20'</span><br>headers = {<br>    <span class="hljs-string">'User-Agent'</span>: <span class="hljs-string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'</span><br>}<br><br><span class="hljs-comment"># (1) 请求对象的定制</span><br>request = urllib.request.Request(url=url, headers=headers)<br><br><span class="hljs-comment"># （2）获取响应的数据</span><br>response = urllib.request.urlopen(request)<br>content = response.read().decode(<span class="hljs-string">'utf-8'</span>)<br><span class="hljs-built_in">print</span>(content)<br><br><span class="hljs-comment"># (3) 数据下载到本地</span><br><span class="hljs-comment"># open方法默认情况下使用的是gbk的编码  如果我们要想保存汉字 那么需要在open方法中指定编码格式为utf-8</span><br>filepath = <span class="hljs-string">'./download/'</span><br><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(filepath):<br>    os.makedirs(filepath)<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filepath + <span class="hljs-string">'豆瓣.json'</span>, <span class="hljs-string">'w'</span>, encoding=<span class="hljs-string">'utf-8'</span>) <span class="hljs-keyword">as</span> f:<br>    f.write(content)<br></code></pre></td></tr></tbody></table></figure><p>运行结果：</p><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220815205456.png" srcset="/img/loading.gif" lazyload alt="image-20220815205456222"></p><h4 id="案例：爬取任意页数豆瓣电影"><a href="#案例：爬取任意页数豆瓣电影" class="headerlink" title="案例：爬取任意页数豆瓣电影"></a>案例：爬取任意页数豆瓣电影</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> urllib.parse<br><span class="hljs-keyword">import</span> urllib.request<br><br><br><span class="hljs-comment"># 创建request</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">creat_request</span>(<span class="hljs-params">page</span>):<br>    base_url = <span class="hljs-string">'https://movie.douban.com/j/chart/top_list?type=10&amp;interval_id=100%3A90&amp;action=&amp;'</span><br>    data = {<br>        <span class="hljs-string">'start'</span>: (page - <span class="hljs-number">1</span>) * <span class="hljs-number">20</span>,<br>        <span class="hljs-string">'limit'</span>: <span class="hljs-number">20</span><br>    }<br>    headers = {<br>        <span class="hljs-string">'User-Agent'</span>: <span class="hljs-string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'</span><br>    }<br>    url = base_url + urllib.parse.urlencode(data)<br>    <span class="hljs-keyword">return</span> urllib.request.Request(url=url, headers=headers)<br><br><br><span class="hljs-comment"># 获取返回值</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_content</span>(<span class="hljs-params">request</span>):<br>    <span class="hljs-keyword">return</span> urllib.request.urlopen(request).read().decode(<span class="hljs-string">'utf-8'</span>)<br><br><br><span class="hljs-comment"># 下载到本地</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">download</span>(<span class="hljs-params">download_page, content</span>):<br>    filepath = <span class="hljs-string">'./download/'</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(filepath):<br>        os.makedirs(filepath)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filepath + <span class="hljs-string">'豆瓣_'</span> + <span class="hljs-built_in">str</span>(download_page) + <span class="hljs-string">'.json'</span>, <span class="hljs-string">'w'</span>, encoding=<span class="hljs-string">'utf-8'</span>) <span class="hljs-keyword">as</span> fp:<br>        fp.write(content)<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:<br>    start_page = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">input</span>(<span class="hljs-string">'请输入起始的页码:'</span>))<br>    end_page = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">input</span>(<span class="hljs-string">'请输入结束的页面:'</span>))<br>    <span class="hljs-keyword">for</span> page <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(start_page, end_page + <span class="hljs-number">1</span>):<br>        <span class="hljs-comment"># (1) 请求对象的定制</span><br>        request = creat_request(page)<br>        <span class="hljs-comment"># （2）获取响应的数据</span><br>        contend = get_content(request)<br>        <span class="hljs-comment"># (3) 数据下载到本地</span><br>        download(page, contend)<br></code></pre></td></tr></tbody></table></figure><p>运行结果：</p><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220815211806.png" srcset="/img/loading.gif" lazyload alt="image-20220815211806707"></p><p>资源目录：</p><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220815211824.png" srcset="/img/loading.gif" lazyload alt="image-20220815211824118"></p><h3 id="ajax的post请求"><a href="#ajax的post请求" class="headerlink" title="ajax的post请求"></a>ajax 的 post 请求</h3><h4 id="案例：爬取肯德基餐厅信息"><a href="#案例：爬取肯德基餐厅信息" class="headerlink" title="案例：爬取肯德基餐厅信息"></a>案例：爬取肯德基餐厅信息</h4><p>首先在<a target="_blank" rel="noopener" href="http://www.kfc.com.cn/kfccda/storelist/index.aspx">肯德基餐厅信息页面</a>找到请求地址</p><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220815214613.png" srcset="/img/loading.gif" lazyload alt="image-20220815214613816"></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> urllib.parse<br><span class="hljs-keyword">import</span> urllib.request<br><br><br><span class="hljs-comment"># 创建request</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">creat_request</span>(<span class="hljs-params">city, pageindex</span>):<br>    url = <span class="hljs-string">'http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=cname'</span><br>    data = {<br>        <span class="hljs-string">'cname'</span>: city,<br>        <span class="hljs-string">'pageindex'</span>: pageindex,<br>        <span class="hljs-string">'pagesize'</span>: <span class="hljs-number">10</span><br>    }<br>    headers = {<br>        <span class="hljs-string">'User-Agent'</span>: <span class="hljs-string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'</span><br>    }<br>    data = urllib.parse.urlencode(data).encode(<span class="hljs-string">'utf-8'</span>)<br>    <span class="hljs-keyword">return</span> urllib.request.Request(url=url, data=data, headers=headers)<br><br><br><span class="hljs-comment"># 获取返回值</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_content</span>(<span class="hljs-params">request</span>):<br>    <span class="hljs-keyword">return</span> urllib.request.urlopen(request).read().decode(<span class="hljs-string">'utf-8'</span>)<br><br><br><span class="hljs-comment"># 下载到本地</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">download</span>(<span class="hljs-params">city, pageindex, content</span>):<br>    filepath = <span class="hljs-string">'./download/'</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(filepath):<br>        os.makedirs(filepath)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filepath + <span class="hljs-string">'KFC_'</span> + city + <span class="hljs-string">'_'</span> + <span class="hljs-built_in">str</span>(pageindex) + <span class="hljs-string">'.json'</span>, <span class="hljs-string">'w'</span>, encoding=<span class="hljs-string">'utf-8'</span>) <span class="hljs-keyword">as</span> fp:<br>        fp.write(content)<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:<br>    city = <span class="hljs-built_in">input</span>(<span class="hljs-string">'请输入需要查询的城市:'</span>)<br>    start_page = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">input</span>(<span class="hljs-string">'请输入起始的页码:'</span>))<br>    end_page = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">input</span>(<span class="hljs-string">'请输入结束的页面:'</span>))<br>    <span class="hljs-keyword">for</span> page <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(start_page, end_page + <span class="hljs-number">1</span>):<br>        <span class="hljs-comment"># (1) 请求对象的定制</span><br>        request = creat_request(city, page)<br>        <span class="hljs-comment"># （2）获取响应的数据</span><br>        contend = get_content(request)<br>        <span class="hljs-comment"># (3) 数据下载到本地</span><br>        download(city, page, contend)<br></code></pre></td></tr></tbody></table></figure><p>运行结果：</p><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220815214652.png" srcset="/img/loading.gif" lazyload alt="image-20220815214652509"></p><p>资源目录：</p><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220815214714.png" srcset="/img/loading.gif" lazyload alt="image-20220815214714316"></p><h2 id="urllib异常"><a href="#urllib异常" class="headerlink" title="urllib异常"></a>urllib 异常</h2><p>在我们的代码中，为了使其健壮性更好，我们可以使用异常捕获 <code>try-except</code> 语句来捕获异常</p><p>urllib 发送请求的过程中，有可能出现两种异常，分别为：</p><ul><li>URLError</li><li>HTTPError</li></ul><h2 id="cookie登录"><a href="#cookie登录" class="headerlink" title="cookie登录"></a>cookie 登录</h2><p>cookie 中携带者个人登录信息，如果有登录之后的 cookie，那么我们可以携带者 cookie 进入到任何页面</p><p>适用的场景：数据采集的时候 需要绕过登陆 然后进入到某个页面</p><p><strong>什么情况下访问不成功？</strong></p><p>因为请求头的信息不够 所以访问不成功</p><h3 id="案例：利用cookie登录微博，并下载详细信息页面"><a href="#案例：利用cookie登录微博，并下载详细信息页面" class="headerlink" title="案例：利用cookie登录微博，并下载详细信息页面"></a>案例：利用 cookie 登录微博，并下载详细信息页面</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> urllib.request<br><br>url = <span class="hljs-string">'https://weibo.cn/7752848280/info'</span><br><br>headers = {<br>    <span class="hljs-string">'authority'</span>: <span class="hljs-string">'weibo.cn'</span>,<br>    <span class="hljs-string">'method'</span>: <span class="hljs-string">'GET'</span>,<br>    <span class="hljs-string">'path'</span>: <span class="hljs-string">'/6451491586/info'</span>,<br>    <span class="hljs-string">'scheme'</span>: <span class="hljs-string">'https'</span>,<br>    <span class="hljs-string">'accept'</span>: <span class="hljs-string">'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'</span>,<br>    <span class="hljs-comment"># 'accept-encoding': 'gzip, deflate, br',</span><br>    <span class="hljs-string">'accept-language'</span>: <span class="hljs-string">'zh-CN,zh;q=0.9'</span>,<br>    <span class="hljs-string">'cache-control'</span>: <span class="hljs-string">'max-age=0'</span>,<br>    <span class="hljs-comment"># cookie中携带着你的登陆信息   如果有登陆之后的cookie  那么我们就可以携带着cookie进入到任何页面</span><br>    <span class="hljs-string">'cookie'</span>: <span class="hljs-string">'xxx'</span>,<br>    <span class="hljs-comment"># referer  判断当前路径是不是由上一个路径进来的    一般情况下 是做图片防盗链</span><br>    <span class="hljs-string">'referer'</span>: <span class="hljs-string">'https://weibo.cn/'</span>,<br>    <span class="hljs-string">'sec-ch-ua'</span>: <span class="hljs-string">'"Chromium";v="92", " Not A;Brand";v="99", "Google Chrome";v="92"'</span>,<br>    <span class="hljs-string">'sec-ch-ua-mobile'</span>: <span class="hljs-string">'?0'</span>,<br>    <span class="hljs-string">'sec-fetch-dest'</span>: <span class="hljs-string">'document'</span>,<br>    <span class="hljs-string">'sec-fetch-mode'</span>: <span class="hljs-string">'navigate'</span>,<br>    <span class="hljs-string">'sec-fetch-site'</span>: <span class="hljs-string">'same-origin'</span>,<br>    <span class="hljs-string">'sec-fetch-user'</span>: <span class="hljs-string">'?1'</span>,<br>    <span class="hljs-string">'upgrade-insecure-requests'</span>: <span class="hljs-string">'1'</span>,<br>    <span class="hljs-string">'user-agent'</span>: <span class="hljs-string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'</span>,<br>}<br><span class="hljs-comment"># 请求对象的定制</span><br>request = urllib.request.Request(url=url, headers=headers)<br><span class="hljs-comment"># 模拟浏览器向服务器发送请求</span><br>response = urllib.request.urlopen(request)<br><span class="hljs-comment"># 获取响应的数据</span><br>content = response.read().decode(<span class="hljs-string">'utf-8'</span>)<br><br><span class="hljs-comment"># 将数据保存到本地</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">'weibo.html'</span>, <span class="hljs-string">'w'</span>, encoding=<span class="hljs-string">'utf-8'</span>) <span class="hljs-keyword">as</span> f:<br>    f.write(content)<br></code></pre></td></tr></tbody></table></figure><p>运行结果：</p><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220815231641.png" srcset="/img/loading.gif" lazyload alt="image-20220815231641103"></p><h2 id="handler-处理器"><a href="#handler-处理器" class="headerlink" title="handler 处理器"></a>handler 处理器</h2><ul><li>目的是用来定制更高级的请求头，随着业务逻辑的复杂，请求对象的定制已经满足不了我们的需求（例如：动态 cookie 和代理不能使用请求对象定制）</li><li>对比：<ul><li><code>urllib.request.urlopen (url)</code> 不能定制请求头；</li><li><code>urllib.request.Request ()</code> 可以定制请求头</li></ul></li></ul><h3 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 需求 使用handler来访问百度  获取网页源码</span><br><br><span class="hljs-keyword">import</span> urllib.request<br><br>url = <span class="hljs-string">'http://www.baidu.com'</span><br><br>headers = {<br>    <span class="hljs-string">'User-Agent'</span>: <span class="hljs-string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'</span><br>}<br><br>request = urllib.request.Request(url=url, headers=headers)<br><br><span class="hljs-comment"># handler   build_opener  open</span><br><br><span class="hljs-comment"># （1）获取hanlder对象</span><br>handler = urllib.request.HTTPHandler()<br><br><span class="hljs-comment"># （2）获取opener对象</span><br>opener = urllib.request.build_opener(handler)<br><br><span class="hljs-comment"># (3) 调用open方法</span><br>response = opener.<span class="hljs-built_in">open</span>(request)<br><br>content = response.read().decode(<span class="hljs-string">'utf-8'</span>)<br><br><span class="hljs-built_in">print</span>(content)<br></code></pre></td></tr></tbody></table></figure><h2 id="代理"><a href="#代理" class="headerlink" title="代理"></a>代理</h2><h3 id="代理服务器"><a href="#代理服务器" class="headerlink" title="代理服务器"></a>代理服务器</h3><ul><li>代理服务器的常用功能：<ul><li>突破自身 ip 访问限制，访问国外节点</li><li>访问一些单位和集体的内部资源</li><li>提高访问速度</li><li>隐藏真实 ip</li></ul></li><li>代码配置代理<ul><li>创建 <code>request</code> 对象</li><li>创建 <code>proxyHandler</code> 对象</li><li>用 <code>handler</code> 对象创建 opener 对象</li><li>使用 <code>opener.open</code> 函数发送请求</li></ul></li></ul><h4 id="案例：使用代理服务器查看ip"><a href="#案例：使用代理服务器查看ip" class="headerlink" title="案例：使用代理服务器查看ip"></a>案例：使用代理服务器查看 ip</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> urllib.request<br><br>url = <span class="hljs-string">'http://www.baidu.com/s?wd=ip'</span><br><br>headers = {<br>    <span class="hljs-string">'Cookie'</span>: <span class="hljs-string">'xxx'</span>,<br>    <span class="hljs-string">'User-Agent'</span>: <span class="hljs-string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'</span><br>}<br><br><span class="hljs-comment"># 请求对象的定制</span><br>request = urllib.request.Request(url=url, headers=headers)<br><br><span class="hljs-comment"># 模拟浏览器访问服务器</span><br><span class="hljs-comment"># response = urllib.request.urlopen(request)</span><br><br>proxies = {<br>    <span class="hljs-string">'http'</span>: <span class="hljs-string">'139.9.64.238:443'</span><br>}<br><span class="hljs-comment"># handler  build_opener  open</span><br>handler = urllib.request.ProxyHandler(proxies=proxies)<br><br>opener = urllib.request.build_opener(handler)<br><br>response = opener.<span class="hljs-built_in">open</span>(request)<br><br><span class="hljs-comment"># 获取响应的信息</span><br>content = response.read().decode(<span class="hljs-string">'utf-8'</span>)<br><br><span class="hljs-comment"># 保存</span><br>filepath = <span class="hljs-string">'./download/'</span><br><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(filepath):<br>    os.makedirs(filepath)<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filepath + <span class="hljs-string">'proxy.html'</span>, <span class="hljs-string">'w'</span>, encoding=<span class="hljs-string">'utf-8'</span>) <span class="hljs-keyword">as</span> fp:<br>    fp.write(content)<br></code></pre></td></tr></tbody></table></figure><p>运行结果：</p><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220816002145.png" srcset="/img/loading.gif" lazyload alt="image-20220816002144984"></p><p>打开页面：发现 ip 已经改变</p><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220816002207.png" srcset="/img/loading.gif" lazyload alt="image-20220816002207220"></p><h3 id="代理池"><a href="#代理池" class="headerlink" title="代理池"></a>代理池</h3><p>当我们的代理 IP 高频次访问某一网站时，有可能使代理 IP 被封杀，所以我们需要使用代理池随机 IP 来避免 IP 被封杀。</p><h4 id="案例：模拟使用代理池"><a href="#案例：模拟使用代理池" class="headerlink" title="案例：模拟使用代理池"></a>案例：模拟使用代理池</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> urllib.request<br><br>proxies_pool = [<br>    {<span class="hljs-string">'http'</span>: <span class="hljs-string">'183.239.62.251:9091'</span>},<br>    {<span class="hljs-string">'http'</span>: <span class="hljs-string">'221.5.80.66:3128'</span>}<br>]<br><br>proxies = random.choice(proxies_pool)<br><br>url = <span class="hljs-string">'http://www.baidu.com/s?wd=ip'</span><br><br>headers = {<br>    <span class="hljs-string">'Cookie'</span>: <span class="hljs-string">'xxx'</span>,<br>    <span class="hljs-string">'User-Agent'</span>: <span class="hljs-string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'</span><br>}<br><br>request = urllib.request.Request(url=url, headers=headers)<br><br>handler = urllib.request.ProxyHandler(proxies=proxies)<br><br>opener = urllib.request.build_opener(handler)<br><br>response = opener.<span class="hljs-built_in">open</span>(request)<br><br>content = response.read().decode(<span class="hljs-string">'utf-8'</span>)<br><br>filepath = <span class="hljs-string">'./download/'</span><br><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(filepath):<br>    os.makedirs(filepath)<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filepath + <span class="hljs-string">'proxy.html'</span>, <span class="hljs-string">'w'</span>, encoding=<span class="hljs-string">'utf-8'</span>) <span class="hljs-keyword">as</span> fp:<br>    fp.write(content)<br></code></pre></td></tr></tbody></table></figure><p>运行结果：</p><p>第一个 IP：</p><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220816002939.png" srcset="/img/loading.gif" lazyload alt="image-20220816002939719"></p><p>第二个 IP：</p><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220816003107.png" srcset="/img/loading.gif" lazyload alt="image-20220816003107554"></p><h1 id="解析数据"><a href="#解析数据" class="headerlink" title="解析数据"></a>解析数据</h1><h2 id="XPath解析"><a href="#XPath解析" class="headerlink" title="XPath解析"></a>XPath 解析</h2><ol><li><p>首先在 chrmoe 浏览器上安装 <a target="_blank" rel="noopener" href="https://chrome.google.com/webstore/detail/xpath-helper/hgimnogjllphhhkhlmebbmlgjoejdpjl/related?utm_source=chrome-ntp-icon">XPath 插件</a>（需要魔法）</p></li><li><p>安装 lxml 库</p><p><code>pip install lxml -i https://pypi.tuna.tsinghua.edu.cn/simple</code></p></li><li><p>导入 <code>lxml.etree</code></p><p><code>from lxml import etree</code></p></li><li><p><code>etree.parse()</code> 解析本地文件</p><p><code>html_tree = etree.parse('XX.html')</code></p></li><li><p><code>etree.HTML()</code> 解析服务器响应文件</p><p><code>html_tree = etree.HTML(response.read().decode('utf‐8')</code></p></li><li><p>解析数据</p><p><code>html_tree.xpath (xpath 路径)</code></p></li></ol><h3 id="XPath语法"><a href="#XPath语法" class="headerlink" title="XPath语法"></a>XPath 语法</h3><ul><li><p>路径查询</p><table><thead><tr><th align="left">表达式</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">nodename</td><td align="left">选取此节点的所有子节点。</td></tr><tr><td align="left">/</td><td align="left">从根节点选取（取子节点）。</td></tr><tr><td align="left">//</td><td align="left">从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置（取子孙节点）。</td></tr><tr><td align="left">.</td><td align="left">选取当前节点。</td></tr><tr><td align="left">..</td><td align="left">选取当前节点的父节点。</td></tr><tr><td align="left">@</td><td align="left">选取属性。</td></tr></tbody></table><p>例子：</p><table><thead><tr><th align="left">路径表达式</th><th align="left">结果</th></tr></thead><tbody><tr><td align="left">bookstore</td><td align="left">选取 bookstore 元素的所有子节点。</td></tr><tr><td align="left">/bookstore</td><td align="left">选取根元素 bookstore。注释：假如路径起始于正斜杠 (/)，则此路径始终代表到某元素的绝对路径！</td></tr><tr><td align="left">bookstore/book</td><td align="left">选取属于 bookstore 的子元素的所有 book 元素。</td></tr><tr><td align="left">//book</td><td align="left">选取所有 book 子元素，而不管它们在文档中的位置。</td></tr><tr><td align="left">bookstore//book</td><td align="left">选择属于 bookstore 元素的后代的所有 book 元素，而不管它们位于 bookstore 之下的什么位置。</td></tr><tr><td align="left">//@lang</td><td align="left">选取名为 lang 的所有属性。</td></tr></tbody></table></li><li><p>谓词查询</p><table><thead><tr><th align="left">路径表达式</th><th align="left">结果</th></tr></thead><tbody><tr><td align="left">/bookstore/book[1]</td><td align="left">选取属于 bookstore 子元素的第一个 book 元素。</td></tr><tr><td align="left">/bookstore/book[last()]</td><td align="left">选取属于 bookstore 子元素的最后一个 book 元素。</td></tr><tr><td align="left">/bookstore/book[last()-1]</td><td align="left">选取属于 bookstore 子元素的倒数第二个 book 元素。</td></tr><tr><td align="left">/bookstore/book[position()&lt;3]</td><td align="left">选取最前面的两个属于 bookstore 元素的子元素的 book 元素。</td></tr><tr><td align="left">//title[@lang]</td><td align="left">选取所有拥有名为 lang 的属性的 title 元素。</td></tr><tr><td align="left">//title[@lang=’eng’]</td><td align="left">选取所有 title 元素，且这些元素拥有值为 eng 的 lang 属性。</td></tr><tr><td align="left">/bookstore/book[price&gt;35.00]</td><td align="left">选取 bookstore 元素的所有 book 元素，且其中的 price 元素的值须大于 35.00。</td></tr><tr><td align="left">/bookstore/book[price&gt;35.00]//title</td><td align="left">选取 bookstore 元素中的 book 元素的所有 title 元素，且其中的 price 元素的值须大于 35.00。</td></tr></tbody></table></li><li><p>模糊查询</p><table><thead><tr><th>表达式</th><th>结果</th></tr></thead><tbody><tr><td>//div[contains(@id, “he”)]</td><td>选取文档中所有 id 属性包含”he“的 div 元素</td></tr><tr><td>//div[starts‐with(@id, “he”)]</td><td>选取文档中所有 id 属性开始于”he“的 div 元素</td></tr><tr><td>//div[ends-with(@id, “he”)]</td><td>选取文档中所有 id 属性以”he“结尾的 div 元素</td></tr></tbody></table></li><li><p>获取标签中的内容</p><table><thead><tr><th>表达式</th><th>结果</th></tr></thead><tbody><tr><td>//div/h1/text()</td><td>选取文档中所有 div 元素下的 h1 元素的内容</td></tr></tbody></table></li><li><p>选取未知节点</p><table><thead><tr><th align="left">通配符</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">*</td><td align="left">匹配任何元素节点。</td></tr><tr><td align="left">@*</td><td align="left">匹配任何属性节点。</td></tr><tr><td align="left">node()</td><td align="left">匹配任何类型的节点。</td></tr></tbody></table><p>例子：</p><table><thead><tr><th align="left">路径表达式</th><th align="left">结果</th></tr></thead><tbody><tr><td align="left">/bookstore/*</td><td align="left">选取 bookstore 元素的所有子元素。</td></tr><tr><td align="left">//*</td><td align="left">选取文档中的所有元素。</td></tr><tr><td align="left">//title[@*]</td><td align="left">选取所有带有属性的 title 元素。</td></tr></tbody></table></li><li><p>选取若干路径</p><table><thead><tr><th align="left">路径表达式</th><th align="left">结果</th></tr></thead><tbody><tr><td align="left">//book/title | //book/price</td><td align="left">选取 book 元素的所有 title 和 price 元素。</td></tr><tr><td align="left">//title | //price</td><td align="left">选取文档中的所有 title 和 price 元素。</td></tr><tr><td align="left">/bookstore/book/title | //price</td><td align="left">选取属于 bookstore 元素的 book 元素的所有 title 元素，以及文档中所有的 price 元素。</td></tr></tbody></table></li></ul><h3 id="XPath内建函数"><a href="#XPath内建函数" class="headerlink" title="XPath内建函数"></a>XPath 内建函数</h3><table><thead><tr><th>函数名称</th><th>xpath 表达式示例</th><th>示例说明</th></tr></thead><tbody><tr><td><code>text()</code></td><td><code>./text()</code></td><td>文本匹配，表示值取当前节点中的文本内容。</td></tr><tr><td><code>contains()</code></td><td><code>//div[contains(@id,'stu')]</code></td><td>模糊匹配，表示选择 id 中包含 “stu” 的所有 div 节点。</td></tr><tr><td><code>last()</code></td><td><code>//*[@class='web'][last()]</code></td><td>位置匹配，表示选择 @class=’web’ 的最后一个节点。</td></tr><tr><td><code>position()</code></td><td><code>//*[@class='site'][position()&lt;=2]</code></td><td>位置匹配，表示选择 @class=’site’ 的前两个节点。</td></tr><tr><td><code>start-with()</code></td><td><code>"//input[start-with(@id,'st')]"</code></td><td>匹配 id 以 st 开头的元素。</td></tr><tr><td><code>ends-with()</code></td><td><code>"//input[ends-with(@id,'st')]"</code></td><td>匹配 id 以 st 结尾的元素。</td></tr><tr><td><code>concat(string1,string2)</code></td><td><code>concat ('C 语言中文网 ',.//*[@class='stie']/@href)</code></td><td>C 语言中文与标签类别属性为 “stie” 的 href 地址做拼接。</td></tr></tbody></table><h3 id="XPath-轴（Axes）"><a href="#XPath-轴（Axes）" class="headerlink" title="XPath 轴（Axes）"></a>XPath 轴（Axes）</h3><table><thead><tr><th align="left">轴名称</th><th align="left">结果</th></tr></thead><tbody><tr><td align="left">ancestor</td><td align="left">选取当前节点的所有先辈（父、祖父等）。</td></tr><tr><td align="left">ancestor-or-self</td><td align="left">选取当前节点的所有先辈（父、祖父等）以及当前节点本身。</td></tr><tr><td align="left">attribute</td><td align="left">选取当前节点的所有属性。</td></tr><tr><td align="left">child</td><td align="left">选取当前节点的所有子元素。</td></tr><tr><td align="left">descendant</td><td align="left">选取当前节点的所有后代元素（子、孙等）。</td></tr><tr><td align="left">descendant-or-self</td><td align="left">选取当前节点的所有后代元素（子、孙等）以及当前节点本身。</td></tr><tr><td align="left">following</td><td align="left">选取文档中当前节点的结束标签之后的所有节点。</td></tr><tr><td align="left">following-sibling</td><td align="left">选取当前节点之后的所有兄弟节点</td></tr><tr><td align="left">namespace</td><td align="left">选取当前节点的所有命名空间节点。</td></tr><tr><td align="left">parent</td><td align="left">选取当前节点的父节点。</td></tr><tr><td align="left">preceding</td><td align="left">选取文档中当前节点的开始标签之前的所有节点。</td></tr><tr><td align="left">preceding-sibling</td><td align="left">选取当前节点之前的所有同级节点。</td></tr><tr><td align="left">self</td><td align="left">选取当前节点。</td></tr></tbody></table><h3 id="XPath-运算符"><a href="#XPath-运算符" class="headerlink" title="XPath 运算符"></a>XPath 运算符</h3><table><thead><tr><th align="left">运算符</th><th align="left">描述</th><th align="left">实例</th><th align="left">返回值</th></tr></thead><tbody><tr><td align="left">|</td><td align="left">计算两个节点集</td><td align="left">//book | //cd</td><td align="left">返回所有拥有 book 和 cd 元素的节点集</td></tr><tr><td align="left">+</td><td align="left">加法</td><td align="left">6 + 4</td><td align="left">10</td></tr><tr><td align="left">-</td><td align="left">减法</td><td align="left">6 - 4</td><td align="left">2</td></tr><tr><td align="left">*</td><td align="left">乘法</td><td align="left">6 * 4</td><td align="left">24</td></tr><tr><td align="left">div</td><td align="left">除法</td><td align="left">8 div 4</td><td align="left">2</td></tr><tr><td align="left">=</td><td align="left">等于</td><td align="left">price=9.80</td><td align="left">如果 price 是 9.80，则返回 true。如果 price 是 9.90，则返回 false。</td></tr><tr><td align="left">!=</td><td align="left">不等于</td><td align="left">price!=9.80</td><td align="left">如果 price 是 9.90，则返回 true。如果 price 是 9.80，则返回 false。</td></tr><tr><td align="left">&lt;</td><td align="left">小于</td><td align="left">price&lt;9.80</td><td align="left">如果 price 是 9.00，则返回 true。如果 price 是 9.90，则返回 false。</td></tr><tr><td align="left">&lt;=</td><td align="left">小于或等于</td><td align="left">price&lt;=9.80</td><td align="left">如果 price 是 9.00，则返回 true。如果 price 是 9.90，则返回 false。</td></tr><tr><td align="left">&gt;</td><td align="left">大于</td><td align="left">price&gt;9.80</td><td align="left">如果 price 是 9.90，则返回 true。如果 price 是 9.80，则返回 false。</td></tr><tr><td align="left">&gt;=</td><td align="left">大于或等于</td><td align="left">price&gt;=9.80</td><td align="left">如果 price 是 9.90，则返回 true。如果 price 是 9.70，则返回 false。</td></tr><tr><td align="left">or</td><td align="left">或</td><td align="left">price=9.80 or price=9.70</td><td align="left">如果 price 是 9.80，则返回 true。如果 price 是 9.50，则返回 false。</td></tr><tr><td align="left">and</td><td align="left">与</td><td align="left">price&gt;9.00 and price&lt;9.90</td><td align="left">如果 price 是 9.80，则返回 true。如果 price 是 8.50，则返回 false。</td></tr><tr><td align="left">mod</td><td align="left">计算除法的余数</td><td align="left">5 mod 2</td><td align="left">1</td></tr></tbody></table><h3 id="案例：获取百度一下"><a href="#案例：获取百度一下" class="headerlink" title="案例：获取百度一下"></a>案例：获取百度一下</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> urllib.request<br><br><span class="hljs-keyword">from</span> lxml <span class="hljs-keyword">import</span> etree<br><br><span class="hljs-comment"># 1 获取网页的源码</span><br><span class="hljs-comment"># 2 解析</span><br><span class="hljs-comment"># 3 打印</span><br><br>url = <span class="hljs-string">'http://www.baidu.com'</span><br>headers = {<br>    <span class="hljs-string">'User-Agent'</span>: <span class="hljs-string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'</span><br>}<br><span class="hljs-comment"># 获取服务器数据</span><br>request = urllib.request.Request(url=url, headers=headers)<br>handler = urllib.request.HTTPHandler()<br>opener = urllib.request.build_opener(handler)<br>response = opener.<span class="hljs-built_in">open</span>(request)<br>content = response.read().decode(<span class="hljs-string">'utf-8'</span>)<br><span class="hljs-comment"># 解析</span><br>tree = etree.HTML(content)<br>resource = tree.xpath(<span class="hljs-string">"//input[@id = 'su']/@value"</span>)<br><span class="hljs-comment"># 打印</span><br><span class="hljs-built_in">print</span>(resource)<br></code></pre></td></tr></tbody></table></figure><p>运行结果：</p><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220816110320.png" srcset="/img/loading.gif" lazyload alt="image-20220816110320088"></p><h3 id="案例：获取站长素材某几页类型的图片"><a href="#案例：获取站长素材某几页类型的图片" class="headerlink" title="案例：获取站长素材某几页类型的图片"></a>案例：获取站长素材某几页类型的图片</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> urllib.parse<br><span class="hljs-keyword">import</span> urllib.request<br><br><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">from</span> lxml <span class="hljs-keyword">import</span> etree<br><br><br><span class="hljs-comment"># 需求：获取前十页图片</span><br><span class="hljs-comment"># 获取代理IP</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_proxy</span>():<br>    proxy = requests.get(<span class="hljs-string">"http://demo.spiderpy.cn//get/"</span>).json().get(<span class="hljs-string">"proxy"</span>)<br>    temp = proxy.split(<span class="hljs-string">':'</span>)<br>    <span class="hljs-keyword">return</span> {temp[<span class="hljs-number">0</span>]: temp[<span class="hljs-number">1</span>]}<br><br><br><span class="hljs-comment"># 创建request</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">creat_request</span>(<span class="hljs-params">url</span>):<br>    headers = {<br>        <span class="hljs-string">'User-Agent'</span>: <span class="hljs-string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'</span><br>    }<br>    <span class="hljs-keyword">return</span> urllib.request.Request(url=url, headers=headers)<br><br><br><span class="hljs-comment"># 创建页面url</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">creat_page_url</span>(<span class="hljs-params">page</span>):<br>    base_url = <span class="hljs-string">'https://sc.chinaz.com/tupian/'</span><br>    <span class="hljs-keyword">if</span> page == <span class="hljs-number">1</span>:<br>        url = base_url + <span class="hljs-string">'index.html'</span><br>    <span class="hljs-keyword">else</span>:<br>        url = base_url + <span class="hljs-string">'index_'</span> + <span class="hljs-built_in">str</span>(page) + <span class="hljs-string">'.html'</span><br>    <span class="hljs-keyword">return</span> url<br><br><br><span class="hljs-comment"># 获取返回值</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_content</span>(<span class="hljs-params"><span class="hljs-built_in">type</span>, request</span>):<br>    handler = urllib.request.ProxyHandler(proxies=get_proxy())<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span> == <span class="hljs-string">'page'</span>:<br>        content = urllib.request.build_opener(handler).<span class="hljs-built_in">open</span>(request).read().decode(<span class="hljs-string">'utf-8'</span>)<br>    <span class="hljs-keyword">elif</span> <span class="hljs-built_in">type</span> == <span class="hljs-string">'img'</span>:<br>        content = urllib.request.build_opener(handler).<span class="hljs-built_in">open</span>(request).read()<br>    <span class="hljs-keyword">if</span> content <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">return</span> content<br><br><br><span class="hljs-comment"># 下载到本地</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">download</span>(<span class="hljs-params">content</span>):<br>    filepath = <span class="hljs-string">'./download/img/'</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(filepath):<br>        os.makedirs(filepath)<br>    tree = etree.HTML(content)<br>    <span class="hljs-comment"># 这里为什么不用src属性，是因为一般设计类图片网站会懒加载，需要在页面中找到能组成url的属性</span><br>    src_list = tree.xpath(<span class="hljs-string">'//body/div[3]/div[2]/div/img/@data-original'</span>)<br>    name_list = tree.xpath(<span class="hljs-string">'//body/div[3]/div[2]/div/img/@alt'</span>)<br><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(src_list)):<br>        src = src_list[i]<br>        name = name_list[i]<br>        url = <span class="hljs-string">'https:'</span> + src.replace(<span class="hljs-string">'_s'</span>, <span class="hljs-string">''</span>)<br>        temp_request = creat_request(url)<br>        temp_content = get_content(<span class="hljs-string">'img'</span>, temp_request)<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filepath + name + <span class="hljs-string">'.jpg'</span>, <span class="hljs-string">'wb'</span>) <span class="hljs-keyword">as</span> fp:<br>            fp.write(temp_content)<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:<br>    start_page = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">input</span>(<span class="hljs-string">'请输入起始的页码:'</span>))<br>    end_page = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">input</span>(<span class="hljs-string">'请输入结束的页面:'</span>))<br>    <span class="hljs-keyword">for</span> page <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(start_page, end_page + <span class="hljs-number">1</span>):<br>        <span class="hljs-comment"># (1) 请求对象的定制</span><br>        page_url = creat_page_url(page)<br>        request = creat_request(page_url)<br>        <span class="hljs-comment"># （2）获取响应的数据</span><br>        contend = get_content(<span class="hljs-string">'page'</span>, request)<br>        <span class="hljs-comment"># (3) 数据下载到本地</span><br>        download(contend)<br></code></pre></td></tr></tbody></table></figure><p>运行结果：</p><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220816144546.png" srcset="/img/loading.gif" lazyload alt="image-20220816144546226"></p><p>资源目录：</p><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220816144624.png" srcset="/img/loading.gif" lazyload alt="image-20220816144624291"></p><h2 id="jsonpath解析"><a href="#jsonpath解析" class="headerlink" title="jsonpath解析"></a>jsonpath 解析</h2><p><strong>jsonpath 适用于解析本地文件，不能用于解析服务器响应文件</strong></p><ol><li><p>安装 jsonpath</p><p><code>pip install jsonpath -i https://pypi.tuna.tsinghua.edu.cn/simple</code></p></li><li><p>基本使用</p><ul><li><code>obj=json.load (open ('json 文件 ', 'r', encoding='utf‐8'))</code></li><li><code>res = jsonpath.jsonpath (obj, 'jsonpath 语法 ')</code></li></ul></li></ol><h3 id="jsonpath语法"><a href="#jsonpath语法" class="headerlink" title="jsonpath语法"></a>jsonpath 语法</h3><table><thead><tr><th>XPath</th><th>JsonPath</th><th>说明</th></tr></thead><tbody><tr><td><code>/</code></td><td><code>$</code></td><td>文档根元素</td></tr><tr><td><code>.</code></td><td><code>@</code></td><td>当前元素</td></tr><tr><td><code>/</code></td><td><code>.</code> 或 <code>[]</code></td><td>匹配下级元素</td></tr><tr><td><code>..</code></td><td><code>N/A</code></td><td>匹配上级元素，JsonPath 不支持此操作符</td></tr><tr><td><code>//</code></td><td><code>..</code></td><td>递归匹配所有子元素</td></tr><tr><td><code>*</code></td><td><code>*</code></td><td>通配符，匹配下级元素</td></tr><tr><td><code>@</code></td><td><code>N/A</code></td><td>匹配属性，JsonPath 不支持此操作符</td></tr><tr><td><code>[]</code></td><td><code>[]</code></td><td>下标运算符，根据索引获取元素，<strong>XPath 索引从 1 开始，JsonPath 索引从 0 开始</strong></td></tr><tr><td>`</td><td>`</td><td><code>[,]</code></td></tr><tr><td><code>N/A</code></td><td><code>[start:end:step]</code></td><td>数据切片操作，XPath 不支持</td></tr><tr><td><code>[]</code></td><td><code>?()</code></td><td>过滤表达式</td></tr><tr><td><code>N/A</code></td><td><code>()</code></td><td>脚本表达式，使用底层脚本引擎，XPath 不支持</td></tr><tr><td><code>()</code></td><td><code>N/A</code></td><td>分组，JsonPath 不支持</td></tr></tbody></table><h3 id="案例：jsonpath解析书店"><a href="#案例：jsonpath解析书店" class="headerlink" title="案例：jsonpath解析书店"></a>案例：jsonpath 解析书店</h3><p>json 文档：</p><figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">{</span><br>  <span class="hljs-attr">"store"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span><br>    <span class="hljs-attr">"book"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>      <span class="hljs-punctuation">{</span><br>        <span class="hljs-attr">"category"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"reference"</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"author"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"Nigel Rees"</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"title"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"Sayings of the Century"</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"price"</span><span class="hljs-punctuation">:</span> <span class="hljs-number">8.95</span><br>      <span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span><br>      <span class="hljs-punctuation">{</span><br>        <span class="hljs-attr">"category"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"fiction"</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"author"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"Evelyn Waugh"</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"title"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"Sword of Honour"</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"price"</span><span class="hljs-punctuation">:</span> <span class="hljs-number">12.99</span><br>      <span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span><br>      <span class="hljs-punctuation">{</span><br>        <span class="hljs-attr">"category"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"fiction"</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"author"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"Herman Melville"</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"title"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"Moby Dick"</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"isbn"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"0-553-21311-3"</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"price"</span><span class="hljs-punctuation">:</span> <span class="hljs-number">8.99</span><br>      <span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span><br>      <span class="hljs-punctuation">{</span><br>        <span class="hljs-attr">"category"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"fiction"</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"author"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"J. R. R. Tolkien"</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"title"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"The Lord of the Rings"</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"isbn"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"0-395-19395-8"</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"price"</span><span class="hljs-punctuation">:</span> <span class="hljs-number">22.99</span><br>      <span class="hljs-punctuation">}</span><br>    <span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"bicycle"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span><br>      <span class="hljs-attr">"color"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"red"</span><span class="hljs-punctuation">,</span><br>      <span class="hljs-attr">"price"</span><span class="hljs-punctuation">:</span> <span class="hljs-number">19.95</span><br>    <span class="hljs-punctuation">}</span><br>  <span class="hljs-punctuation">}</span><br><span class="hljs-punctuation">}</span><br></code></pre></td></tr></tbody></table></figure><p>解析结果：</p><table><thead><tr><th>XPath</th><th>JsonPath</th><th>Result</th></tr></thead><tbody><tr><td><code>/store/book/author</code></td><td><code>$.store.book[*].author</code></td><td>所有 book 的 author 节点</td></tr><tr><td><code>//author</code></td><td><code>$..author</code></td><td>所有 author 节点</td></tr><tr><td><code>/store/*</code></td><td><code>$.store.*</code></td><td>store 下的所有节点，book 数组和 bicycle 节点</td></tr><tr><td><code>/store//price</code></td><td><code>$.store..price</code></td><td>store 下的所有 price 节点</td></tr><tr><td><code>//book[3]</code></td><td><code>$..book[2]</code></td><td>匹配第 3 个 book 节点</td></tr><tr><td><code>//book[last()]</code></td><td><code>$..book[(@.length-1)]</code>，或 <code>$..book[-1:]</code></td><td>匹配倒数第 1 个 book 节点</td></tr><tr><td><code>//book[position()&lt;3]</code></td><td><code>$..book[0,1]</code>，或 <code>$..book[:2]</code></td><td>匹配前两个 book 节点</td></tr><tr><td><code>//book[isbn]</code></td><td><code>$..book[?(@.isbn)]</code></td><td>过滤含 isbn 字段的节点</td></tr><tr><td><code>//book[price&lt;10]</code></td><td><code>$..book[?(@.price&lt;10)]</code></td><td>过滤 <code>price&lt;10</code> 的节点</td></tr><tr><td><code>//*</code></td><td><code>$..*</code></td><td>递归匹配所有子节点</td></tr></tbody></table><h3 id="案例：jsonpath解析淘票票城市"><a href="#案例：jsonpath解析淘票票城市" class="headerlink" title="案例：jsonpath解析淘票票城市"></a>案例：jsonpath 解析淘票票城市</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> urllib.request<br><br><span class="hljs-keyword">import</span> jsonpath<br><br>url = <span class="hljs-string">'https://dianying.taobao.com/cityAction.json?activityId&amp;_ksTS=1660664202865_108&amp;jsoncallback=jsonp109&amp;action=cityAction&amp;n_s=new&amp;event_submit_doGetAllRegion=true'</span><br><br>headers = {<br>    <span class="hljs-string">'accept'</span>: <span class="hljs-string">' text/javascript, application/javascript, application/ecmascript, application/x-ecmascript, */*; q=0.01'</span>,<br>    <span class="hljs-string">'accept-language'</span>: <span class="hljs-string">' zh-CN,zh;q=0.9,en;q=0.8'</span>,<br>    <span class="hljs-string">'bx-v'</span>: <span class="hljs-string">' 2.2.2'</span>,<br>    <span class="hljs-string">'cookie'</span>: <span class="hljs-string">' t=a8237d7bd8b3656ea8aec5b43ed1cd2b; cookie2=1a50f8f7485b2b81047571e394163bdb; v=0; _tb_token_=be1ebe8b75e5; cna=RqNlG6se4VoCAXFlN34wp09x; xlly_s=1; tfstk=c7LGBADMUhS1LfRygF_soY_ftEnRZNVNUE-28fGpQRYkWt8FiirU4kEmt_pMko1..; l=eBPf7SMHLqeIcNP3BO5Bhurza779FQdbzPVzaNbMiInca6CRtFMY_NCHSSlkSdtjgt5ffeKPNKfZAdUMoW4_WAsWHpfuKtyuJbJeRe1..; isg=BP__h7U7oc3k4qXROIFYPvgMjtOJ5FOGlW4-cJHPP671oB0imLPn1FT-4nBe_Sv-'</span>,<br>    <span class="hljs-string">'referer'</span>: <span class="hljs-string">'https://dianying.taobao.com/'</span>,<br>    <span class="hljs-string">'sec-ch-ua-mobile'</span>: <span class="hljs-string">' ?0'</span>,<br>    <span class="hljs-string">'sec-fetch-dest'</span>: <span class="hljs-string">' empty'</span>,<br>    <span class="hljs-string">'sec-fetch-mode'</span>: <span class="hljs-string">' cors'</span>,<br>    <span class="hljs-string">'sec-fetch-site'</span>: <span class="hljs-string">' same-origin'</span>,<br>    <span class="hljs-string">'user-agent'</span>: <span class="hljs-string">' Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36'</span>,<br>    <span class="hljs-string">'x-requested-with'</span>: <span class="hljs-string">' XMLHttpRequest'</span>,<br>}<br><br>request = urllib.request.Request(url, headers=headers)<br>response = urllib.request.urlopen(request)<br>content = response.read().decode(<span class="hljs-string">'utf-8'</span>).split(<span class="hljs-string">'('</span>)[<span class="hljs-number">1</span>].split(<span class="hljs-string">')'</span>)[<span class="hljs-number">0</span>]<br>filepath = <span class="hljs-string">'./download/'</span><br><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(filepath):<br>    os.makedirs(filepath)<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filepath + <span class="hljs-string">'淘票票城市.json'</span>, <span class="hljs-string">'w'</span>, encoding=<span class="hljs-string">'utf-8'</span>) <span class="hljs-keyword">as</span> f:<br>    f.write(content)<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filepath + <span class="hljs-string">'淘票票城市.json'</span>, <span class="hljs-string">'r'</span>, encoding=<span class="hljs-string">'utf-8'</span>) <span class="hljs-keyword">as</span> f:<br>    obj = json.loads(f.read())<br>    city_list = jsonpath.jsonpath(obj, <span class="hljs-string">'$..regionName'</span>)<br>    <span class="hljs-built_in">print</span>(city_list)<br></code></pre></td></tr></tbody></table></figure><p>运行结果：</p><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220817000416.png" srcset="/img/loading.gif" lazyload alt="image-20220817000415916"></p><h2 id="BeautifulSoup解析"><a href="#BeautifulSoup解析" class="headerlink" title="BeautifulSoup解析"></a>BeautifulSoup 解析</h2><ol><li><p>基本简介</p><ul><li><p><a target="_blank" rel="noopener" href="https://beautifulsoup.cn/">官方文档</a></p></li><li><p>BeautifulSoup 简称：<strong>bs4</strong></p></li><li><p>什么是 BeatifulSoup？</p><ul><li>BeautifulSoup，和 lxml 一样，是一个 html 的解析器，主要功能也是解析和提取数据</li></ul></li><li><p>优缺点</p><ul><li>缺点：效率没有 lxml 的效率高</li><li>优点：接口设计人性化，使用方便</li></ul></li></ul></li><li><p>安装及创建</p><ul><li>安装<ul><li><code>pip install bs4 -i https://pypi.tuna.tsinghua.edu.cn/simple</code></li></ul></li><li>导入<ul><li><code>from bs4 import BeautifulSoup</code></li></ul></li><li>创建对象<ul><li>服务器响应的文件生成对象<ul><li><code>soup = BeautifulSoup(response.read().decode(), 'lxml')</code></li></ul></li><li>本地文件生成对象<ul><li><code>soup = BeautifulSoup(open('1.html'), 'lxml')</code></li><li>注意：默认打开文件的编码格式 gbk 所以需要指定打开编码格式</li></ul></li></ul></li></ul></li></ol><h3 id="案例：bs4基本使用"><a href="#案例：bs4基本使用" class="headerlink" title="案例：bs4基本使用"></a>案例：bs4 基本使用</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> bs4 <span class="hljs-keyword">import</span> BeautifulSoup<br><br><span class="hljs-comment"># 通过解析本地文件 来将bs4的基础语法进行讲解</span><br><span class="hljs-comment"># 默认打开的文件的编码格式是gbk 所以在打开文件的时候需要指定编码</span><br>soup = BeautifulSoup(<span class="hljs-built_in">open</span>(<span class="hljs-string">"./download/bs4.html"</span>, encoding=<span class="hljs-string">'utf-8'</span>), <span class="hljs-string">'lxml'</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">'-'</span> * <span class="hljs-number">60</span> + <span class="hljs-string">' 根据标签名查找节点 '</span> + <span class="hljs-string">'-'</span> * <span class="hljs-number">60</span>)<br><span class="hljs-comment"># 根据标签名查找节点</span><br><span class="hljs-comment"># 找到的是第一个符合条件的数据</span><br><span class="hljs-built_in">print</span>(soup.a)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">'-'</span> * <span class="hljs-number">60</span> + <span class="hljs-string">' 获取标签的属性和属性值 '</span> + <span class="hljs-string">'-'</span> * <span class="hljs-number">60</span>)<br><span class="hljs-comment"># 获取标签的属性和属性值</span><br><span class="hljs-built_in">print</span>(soup.a.attrs)<br><br><span class="hljs-comment"># bs4的一些函数</span><br><span class="hljs-comment"># （1）find</span><br><span class="hljs-comment"># 返回的是第一个符合条件的数据</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">'-'</span> * <span class="hljs-number">60</span> + <span class="hljs-string">' find '</span> + <span class="hljs-string">'-'</span> * <span class="hljs-number">60</span>)<br><span class="hljs-built_in">print</span>(soup.find(<span class="hljs-string">'a'</span>))<br><br><span class="hljs-comment"># 根据title的值来找到对应的标签对象</span><br><span class="hljs-built_in">print</span>(soup.find(<span class="hljs-string">'a'</span>, title=<span class="hljs-string">"a2"</span>))<br><br><span class="hljs-comment"># 根据class的值来找到对应的标签对象  注意的是class需要添加下划线</span><br><span class="hljs-built_in">print</span>(soup.find(<span class="hljs-string">'a'</span>, class_=<span class="hljs-string">"a1"</span>))<br><br><span class="hljs-comment"># （2）find_all  返回的是一个列表 并且返回了所有的a标签</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">'-'</span> * <span class="hljs-number">60</span> + <span class="hljs-string">' find_all '</span> + <span class="hljs-string">'-'</span> * <span class="hljs-number">60</span>)<br><span class="hljs-built_in">print</span>(soup.find_all(<span class="hljs-string">'a'</span>))<br><br><span class="hljs-comment"># 如果想获取的是多个标签的数据 那么需要在find_all的参数中添加的是列表的数据</span><br><span class="hljs-built_in">print</span>(soup.find_all([<span class="hljs-string">'a'</span>, <span class="hljs-string">'span'</span>]))<br><br><span class="hljs-comment"># limit的作用是查找前几个数据</span><br><span class="hljs-built_in">print</span>(soup.find_all(<span class="hljs-string">'li'</span>, limit=<span class="hljs-number">2</span>))<br><br><span class="hljs-comment"># （3）select（推荐）</span><br><span class="hljs-comment"># select方法返回的是一个列表  并且会返回多个数据</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">'-'</span> * <span class="hljs-number">60</span> + <span class="hljs-string">' select '</span> + <span class="hljs-string">'-'</span> * <span class="hljs-number">60</span>)<br><span class="hljs-built_in">print</span>(soup.select(<span class="hljs-string">'a'</span>))<br><br><span class="hljs-comment"># 可以通过.代表class  我们把这种操作叫做类选择器</span><br><span class="hljs-built_in">print</span>(soup.select(<span class="hljs-string">'.a1'</span>))<br><br><span class="hljs-comment"># 可以通过#代表id 我们把这种操作叫做id选择器</span><br><span class="hljs-built_in">print</span>(soup.select(<span class="hljs-string">'#l1'</span>))<br><br><span class="hljs-comment"># 属性选择器---通过属性来寻找对应的标签</span><br><span class="hljs-comment"># 查找到li标签中有id的标签</span><br><span class="hljs-built_in">print</span>(soup.select(<span class="hljs-string">'li[id]'</span>))<br><br><span class="hljs-comment"># 查找到li标签中id为l2的标签</span><br><span class="hljs-built_in">print</span>(soup.select(<span class="hljs-string">'li[id="l2"]'</span>))<br><br><span class="hljs-comment"># 层级选择器</span><br><span class="hljs-comment"># 后代选择器</span><br><span class="hljs-comment"># 找到的是div下面的li</span><br><span class="hljs-built_in">print</span>(soup.select(<span class="hljs-string">'div li'</span>))<br><br><span class="hljs-comment"># 子代选择器</span><br><span class="hljs-comment"># 某标签的第一级子标签</span><br><span class="hljs-comment"># 注意：很多的计算机编程语言中 如果不加空格不会输出内容  但是在bs4中 不会报错 会显示内容</span><br><span class="hljs-built_in">print</span>(soup.select(<span class="hljs-string">'div &gt; ul &gt; li'</span>))<br><br><span class="hljs-comment"># 找到a标签和li标签的所有的对象</span><br><span class="hljs-built_in">print</span>(soup.select(<span class="hljs-string">'a,li'</span>))<br><br><span class="hljs-comment"># 节点信息</span><br><span class="hljs-comment"># 获取节点内容</span><br>obj = soup.select(<span class="hljs-string">'#d1'</span>)[<span class="hljs-number">0</span>]<br><span class="hljs-comment"># 如果标签对象中 只有内容 那么string和get_text()都可以使用</span><br><span class="hljs-comment"># 如果标签对象中 除了内容还有标签 那么string就获取不到数据 而get_text()是可以获取数据</span><br><span class="hljs-comment"># 我们一般情况下  推荐使用get_text()</span><br><span class="hljs-built_in">print</span>(obj.string)<br><span class="hljs-built_in">print</span>(obj.get_text())<br><br><span class="hljs-comment"># 节点的属性</span><br>obj = soup.select(<span class="hljs-string">'#p1'</span>)[<span class="hljs-number">0</span>]<br><span class="hljs-comment"># name是标签的名字</span><br><span class="hljs-built_in">print</span>(obj.name)<br><span class="hljs-comment"># 将属性值左右一个字典返回</span><br><span class="hljs-built_in">print</span>(obj.attrs)<br><br><span class="hljs-comment"># 获取节点的属性</span><br>obj = soup.select(<span class="hljs-string">'#p1'</span>)[<span class="hljs-number">0</span>]<br><br><span class="hljs-built_in">print</span>(obj.attrs.get(<span class="hljs-string">'class'</span>))<br><span class="hljs-built_in">print</span>(obj.get(<span class="hljs-string">'class'</span>))<br><span class="hljs-built_in">print</span>(obj[<span class="hljs-string">'class'</span>])<br></code></pre></td></tr></tbody></table></figure><p>html 页面：</p><figure class="highlight html"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-meta">&lt;!DOCTYPE <span class="hljs-keyword">html</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">html</span> <span class="hljs-attr">lang</span>=<span class="hljs-string">"en"</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">head</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">meta</span> <span class="hljs-attr">charset</span>=<span class="hljs-string">"UTF-8"</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">title</span>&gt;</span>Title<span class="hljs-tag">&lt;/<span class="hljs-name">title</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">head</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">body</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">div</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">ul</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">li</span> <span class="hljs-attr">id</span>=<span class="hljs-string">"l1"</span>&gt;</span>张三<span class="hljs-tag">&lt;/<span class="hljs-name">li</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">li</span> <span class="hljs-attr">id</span>=<span class="hljs-string">"l2"</span>&gt;</span>李四<span class="hljs-tag">&lt;/<span class="hljs-name">li</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">li</span>&gt;</span>王五<span class="hljs-tag">&lt;/<span class="hljs-name">li</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">a</span> <span class="hljs-attr">class</span>=<span class="hljs-string">"a1"</span> <span class="hljs-attr">href</span>=<span class="hljs-string">"https://www.baidu.com"</span> <span class="hljs-attr">title</span>=<span class="hljs-string">"a1"</span>&gt;</span>百度<span class="hljs-tag">&lt;/<span class="hljs-name">a</span>&gt;</span><br>            <span class="hljs-tag">&lt;/<span class="hljs-name">ul</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">ol</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">li</span>&gt;</span>测试<span class="hljs-tag">&lt;/<span class="hljs-name">li</span>&gt;</span><br>            <span class="hljs-tag">&lt;/<span class="hljs-name">ol</span>&gt;</span><br>        <span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">a</span> <span class="hljs-attr">class</span>=<span class="hljs-string">"a2"</span> <span class="hljs-attr">href</span>=<span class="hljs-string">"https://www.taobao.com"</span> <span class="hljs-attr">title</span>=<span class="hljs-string">"a2"</span>&gt;</span>淘宝<span class="hljs-tag">&lt;/<span class="hljs-name">a</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">span</span> <span class="hljs-attr">class</span>=<span class="hljs-string">"p1"</span> <span class="hljs-attr">id</span>=<span class="hljs-string">"p1"</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">h1</span>&gt;</span>11<span class="hljs-tag">&lt;/<span class="hljs-name">h1</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">span</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">id</span>=<span class="hljs-string">"d1"</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">span</span>&gt;</span>哈哈哈<span class="hljs-tag">&lt;/<span class="hljs-name">span</span>&gt;</span><br>        <span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">body</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">html</span>&gt;</span><br></code></pre></td></tr></tbody></table></figure><p>运行结果：</p><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220817010423.png" srcset="/img/loading.gif" lazyload alt="image-20220817010422922"></p><h3 id="案例：bs4爬取星巴克菜单"><a href="#案例：bs4爬取星巴克菜单" class="headerlink" title="案例：bs4爬取星巴克菜单"></a>案例：bs4 爬取星巴克菜单</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> urllib.request<br><br><span class="hljs-keyword">from</span> bs4 <span class="hljs-keyword">import</span> BeautifulSoup<br><br>url = <span class="hljs-string">'https://www.starbucks.com.cn/menu/'</span><br><br>response = urllib.request.urlopen(url)<br><br>content = response.read().decode(<span class="hljs-string">'utf-8'</span>)<br><br>soup = BeautifulSoup(content, <span class="hljs-string">'lxml'</span>)<br><br><span class="hljs-comment"># //ul[@class="grid padded-3 product"]//strong/text()</span><br><span class="hljs-comment"># name_list = soup.select('ul[class="grid padded-3 product"] strong')</span><br>name_list = soup.select(<span class="hljs-string">'.grid.padded-3.product strong'</span>)<br><br><span class="hljs-keyword">for</span> name <span class="hljs-keyword">in</span> name_list:<br>    <span class="hljs-built_in">print</span>(name.get_text())<br></code></pre></td></tr></tbody></table></figure><p>运行结果：</p><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220817011240.png" srcset="/img/loading.gif" lazyload alt="image-20220817011240686"></p><h1 id="Selenium"><a href="#Selenium" class="headerlink" title="Selenium"></a>Selenium</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><ul><li>Selenium 是一个<strong>用于 Web 应用程序测试</strong>的工具。</li><li>Selenium 测试<strong>直接运行在浏览器中</strong>，就像真正的用户在操作一样。</li><li>支持通过各种 <strong>driver</strong>（FirfoxDriver，IternetExplorerDriver，OperaDriver，ChromeDriver）驱动<strong>真实浏览器</strong>完成测试。</li><li>selenium 也是<strong>支持无界面浏览器</strong>操作的。</li></ul><h2 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h2><p>模拟浏览器功能，自动执行网页中的 js 代码，实现动态加载。</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>安装 Selenium：<code>pip install selenium -i https://pypi.tuna.tsinghua.edu.cn/simple</code></p><p>驱动下载 **(一定注意与浏览器版本对应)**：</p><ul><li>Chrome 浏览器驱动：<a target="_blank" rel="noopener" href="https://sites.google.com/a/chromium.org/chromedriver/home">chromedriver</a> , <a target="_blank" rel="noopener" href="http://chromedriver.storage.googleapis.com/index.html">备用地址</a></li><li>Firefox 浏览器驱动：<a target="_blank" rel="noopener" href="https://github.com/mozilla/geckodriver/releases">geckodriver</a></li><li>Edge 浏览器驱动：<a target="_blank" rel="noopener" href="https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver">MicrosoftWebDriver</a></li><li>IE 浏览器驱动：<a target="_blank" rel="noopener" href="http://selenium-release.storage.googleapis.com/index.html">IEDriverServer</a></li><li>Opera 浏览器驱动：<a target="_blank" rel="noopener" href="https://github.com/operasoftware/operachromiumdriver/releases">operadriver</a></li><li>PhantomJS 浏览器驱动：<a target="_blank" rel="noopener" href="http://phantomjs.org/">phantomjs</a></li></ul><h2 id="Driver-真实浏览器"><a href="#Driver-真实浏览器" class="headerlink" title="Driver+真实浏览器"></a>Driver + 真实浏览器</h2><h3 id="使用步骤"><a href="#使用步骤" class="headerlink" title="使用步骤"></a>使用步骤</h3><ol><li><p>下载驱动并解压，这里使用 chrome。</p></li><li><p>导入：<code>from selenium import webdriver</code></p></li><li><p>创建谷歌浏览器操作对象：</p><ul><li><p>如果把 chromedriver.exe 放在 其他目录下，需要使用路径</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs pyt">path = './res/chromedriver.exe'<br>browser = webdriver.Chrome(service=Service(path))<br></code></pre></td></tr></tbody></table></figure></li><li><p>如果把 chromedriver.exe 放在 脚本文件执行目录下</p><p><code>browser = webdriver.Chrome()</code></p></li></ul></li><li><p>访问网址</p><ul><li><code>url = ‘要访问的网址’ browser.get (url)</code></li></ul></li></ol><h3 id="案例：获取京东带有秒杀板块的源码"><a href="#案例：获取京东带有秒杀板块的源码" class="headerlink" title="案例：获取京东带有秒杀板块的源码"></a>案例：获取京东带有秒杀板块的源码</h3><p>在之前的方法中获取到的京东首页源码，会被反爬机制认定不是真实浏览器，从而失去秒杀板块的源码。</p><p>我们使用 selenium 可以避免出现这种情况</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># （1）导入selenium</span><br><span class="hljs-keyword">from</span> selenium <span class="hljs-keyword">import</span> webdriver<br><span class="hljs-keyword">from</span> selenium.webdriver.chrome.service <span class="hljs-keyword">import</span> Service<br><br><span class="hljs-comment"># (2) 创建浏览器操作对象</span><br><span class="hljs-comment"># 如果把 chromedriver.exe 放在 其他目录下，需要使用路径</span><br>path = <span class="hljs-string">'./res/chromedriver.exe'</span><br>browser = webdriver.Chrome(service=Service(path))<br><span class="hljs-comment"># 如果把 chromedriver.exe 放在 脚本文件执行目录下</span><br><span class="hljs-comment"># browser = webdriver.Chrome()</span><br><br><span class="hljs-comment"># （3）访问网站</span><br>url = <span class="hljs-string">'https://www.jd.com/'</span><br>browser.get(url)<br><br><span class="hljs-comment"># page_source获取网页源码</span><br>content = browser.page_source<br><span class="hljs-built_in">print</span>(content)<br></code></pre></td></tr></tbody></table></figure><h3 id="案例：元素定位"><a href="#案例：元素定位" class="headerlink" title="案例：元素定位"></a>案例：元素定位</h3><p>使用 selenium 定位页面元素</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> selenium <span class="hljs-keyword">import</span> webdriver<br><span class="hljs-keyword">from</span> selenium.webdriver.chrome.service <span class="hljs-keyword">import</span> Service<br><span class="hljs-keyword">from</span> selenium.webdriver.common.by <span class="hljs-keyword">import</span> By<br><br><span class="hljs-comment"># 如果把 chromedriver.exe 放在 其他目录下，需要使用路径</span><br>path = <span class="hljs-string">'./res/chromedriver.exe'</span><br>browser = webdriver.Chrome(service=Service(path))<br><span class="hljs-comment"># 如果把 chromedriver.exe 放在 脚本文件执行目录下</span><br><span class="hljs-comment"># browser = webdriver.Chrome()</span><br><br>url = <span class="hljs-string">'https://www.baidu.com'</span><br>browser.get(url)<br><br><span class="hljs-comment"># 元素定位</span><br><br><span class="hljs-comment"># 根据id来找到对象</span><br>button = browser.find_element(By.ID, <span class="hljs-string">'su'</span>)<br><span class="hljs-built_in">print</span>(button)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">'-'</span> * <span class="hljs-number">120</span>)<br><br><span class="hljs-comment"># 根据标签属性的属性值来获取对象的</span><br>button = browser.find_element(By.NAME, <span class="hljs-string">'wd'</span>)<br><span class="hljs-built_in">print</span>(button)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">'-'</span> * <span class="hljs-number">120</span>)<br><br><span class="hljs-comment"># 根据xpath语句来获取对象</span><br>button = browser.find_elements(By.XPATH, <span class="hljs-string">'//input[@id="su"]'</span>)<br><span class="hljs-built_in">print</span>(button)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">'-'</span> * <span class="hljs-number">120</span>)<br><br><span class="hljs-comment"># 根据标签的名字来获取对象</span><br>button = browser.find_elements(By.TAG_NAME, <span class="hljs-string">'input'</span>)<br><span class="hljs-built_in">print</span>(button)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">'-'</span> * <span class="hljs-number">120</span>)<br><br><span class="hljs-comment"># 使用的bs4的语法来获取对象</span><br>button = browser.find_elements(By.CSS_SELECTOR, <span class="hljs-string">'#su'</span>)<br><span class="hljs-built_in">print</span>(button)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">'-'</span> * <span class="hljs-number">120</span>)<br><br>button = browser.find_element(By.LINK_TEXT, <span class="hljs-string">'直播'</span>)<br><span class="hljs-built_in">print</span>(button)<br></code></pre></td></tr></tbody></table></figure><h3 id="案例：访问元素信息"><a href="#案例：访问元素信息" class="headerlink" title="案例：访问元素信息"></a>案例：访问元素信息</h3><p>使用 selenium 自动化执行浏览器操作</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> selenium <span class="hljs-keyword">import</span> webdriver<br><span class="hljs-keyword">from</span> selenium.webdriver.chrome.service <span class="hljs-keyword">import</span> Service<br><span class="hljs-keyword">from</span> selenium.webdriver.common.by <span class="hljs-keyword">import</span> By<br><br><span class="hljs-comment"># 如果把 chromedriver.exe 放在 其他目录下，需要使用路径</span><br>path = <span class="hljs-string">'./res/chromedriver.exe'</span><br>browser = webdriver.Chrome(service=Service(path))<br><span class="hljs-comment"># 如果把 chromedriver.exe 放在 脚本文件执行目录下</span><br><span class="hljs-comment"># browser = webdriver.Chrome()</span><br><br><span class="hljs-comment"># url</span><br>url = <span class="hljs-string">'https://www.baidu.com'</span><br>browser.get(url)<br><br><span class="hljs-keyword">import</span> time<br><br>time.sleep(<span class="hljs-number">2</span>)<br><br><span class="hljs-comment"># 获取文本框的对象</span><br><span class="hljs-built_in">input</span> = browser.find_element(By.ID, <span class="hljs-string">'kw'</span>)<br><br><span class="hljs-comment"># 在文本框中输入周杰伦</span><br><span class="hljs-built_in">input</span>.send_keys(<span class="hljs-string">'周杰伦'</span>)<br><br>time.sleep(<span class="hljs-number">2</span>)<br><br><span class="hljs-comment"># 获取百度一下的按钮</span><br>button = browser.find_element(By.ID, <span class="hljs-string">'su'</span>)<br><br><span class="hljs-comment"># 点击按钮</span><br>button.click()<br><br>time.sleep(<span class="hljs-number">2</span>)<br><br><span class="hljs-comment"># 滑到底部</span><br>js_bottom = <span class="hljs-string">'document.documentElement.scrollTop=100000'</span><br>browser.execute_script(js_bottom)<br><br>time.sleep(<span class="hljs-number">2</span>)<br><br><span class="hljs-comment"># 获取下一页的按钮</span><br><span class="hljs-built_in">next</span> = browser.find_element(By.XPATH, <span class="hljs-string">'//a[@class="n"]'</span>)<br><br><span class="hljs-comment"># 点击下一页</span><br><span class="hljs-built_in">next</span>.click()<br><br>time.sleep(<span class="hljs-number">2</span>)<br><br><span class="hljs-comment"># 回到上一页</span><br>browser.back()<br><br>time.sleep(<span class="hljs-number">2</span>)<br><br><span class="hljs-comment"># 回去</span><br>browser.forward()<br><br>time.sleep(<span class="hljs-number">3</span>)<br><br><span class="hljs-comment"># 退出</span><br>browser.quit()<br></code></pre></td></tr></tbody></table></figure><h2 id="Driver-Phantomjs"><a href="#Driver-Phantomjs" class="headerlink" title="Driver+Phantomjs"></a>Driver+Phantomjs</h2><h4 id="什么是-Phantomjs"><a href="#什么是-Phantomjs" class="headerlink" title="什么是 Phantomjs"></a>什么是 Phantomjs</h4><ul><li><p>是一个无界面的浏览器</p></li><li><p>支持页面元素查找，js 的执行等</p></li><li><p>由于不进行 css 和 gui 渲染，运行效率要比真实的浏览器要快很多</p></li></ul><h4 id="如何使用"><a href="#如何使用" class="headerlink" title="如何使用"></a>如何使用</h4><ol><li><strong>PhantomJS 要求的 selenium 版本较低，需要使用 2.48.0 版本</strong></li><li>获取 PhantomJS.exe 文件路径 path</li><li><code>browser = webdriver.PhantomJS(service=Service(path))</code></li><li>``browser.get (url)`</li><li>扩展：保存屏幕快照：<code>browser.save_screenshot ('baidu.png')</code></li></ol><h4 id="案例：使用selenium操作PhantomJS（未实践）"><a href="#案例：使用selenium操作PhantomJS（未实践）" class="headerlink" title="案例：使用selenium操作PhantomJS（未实践）"></a>案例：使用 selenium 操作 PhantomJS（未实践）</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> selenium <span class="hljs-keyword">import</span> webdriver<br> <br>path = <span class="hljs-string">'phantomjs.exe'</span><br> <br>browser = webdriver.PhantomJS(path)<br> <br> <br>url = <span class="hljs-string">'https://www.baidu.com'</span><br>browser.get(url)<br> <br>browser.save_screenshot(<span class="hljs-string">'baidu.png'</span>)<br> <br><span class="hljs-keyword">import</span> time<br>time.sleep(<span class="hljs-number">2</span>)<br> <br><span class="hljs-built_in">input</span> = browser.find_element_by_id(<span class="hljs-string">'kw'</span>)<br><span class="hljs-built_in">input</span>.send_keys(<span class="hljs-string">'陈奕迅'</span>)<br> <br>time.sleep(<span class="hljs-number">3</span>)<br> <br>browser.save_screenshot(<span class="hljs-string">'陈奕迅.png'</span>)<br></code></pre></td></tr></tbody></table></figure><h2 id="Driver-Chrome-headless"><a href="#Driver-Chrome-headless" class="headerlink" title="Driver+Chrome headless"></a>Driver+Chrome headless</h2><p>Chrome-headless 模式， Google 针对 Chrome 浏览器 <strong>59 版</strong> 新增加的一种模式，可以让你不打开 UI 界面的情况下 使用 Chrome 浏览器，所以运行效果与 Chrome 保持完美一致。</p><h3 id="系统要求"><a href="#系统要求" class="headerlink" title="系统要求"></a>系统要求</h3><ul><li>Unix/Linux 系统：系统需要 chrome &gt;= 59</li><li>Windows 系统：系统需要 chrome &gt;= 60</li><li>Python 3.6 以上</li><li>Selenium 3.4 以上</li><li>ChromeDriver 2.31 以上</li></ul><h3 id="案例：使用无头模式获取截图"><a href="#案例：使用无头模式获取截图" class="headerlink" title="案例：使用无头模式获取截图"></a>案例：使用无头模式获取截图</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> selenium <span class="hljs-keyword">import</span> webdriver<br><span class="hljs-keyword">from</span> selenium.webdriver.chrome.options <span class="hljs-keyword">import</span> Options<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_chrome_headless_browser</span>():<br>    chrome_option = Options()<br>    chrome_option.add_argument(<span class="hljs-string">'--headless'</span>)<br>    chrome_option.add_argument(<span class="hljs-string">'--disable-gpu'</span>)<br>    <span class="hljs-keyword">return</span> webdriver.Chrome(options=chrome_option)<br><br><br>browser = create_chrome_headless_browser()<br>url = <span class="hljs-string">'https://www.baidu.com'</span><br>browser.get(url)<br>browser.save_screenshot(<span class="hljs-string">"./download/百度.png"</span>)<br>browser.quit()<br></code></pre></td></tr></tbody></table></figure><h1 id="requests"><a href="#requests" class="headerlink" title="requests"></a>requests</h1><h2 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h2><p><a target="_blank" rel="noopener" href="https://requests.readthedocs.io/projects/cn/zh_CN/latest/">官方文档</a></p><p><a target="_blank" rel="noopener" href="https://requests.readthedocs.io/projects/cn/zh_CN/latest/user/quickstart.html">快速上手</a></p><p><a target="_blank" rel="noopener" href="https://github.com/psf/requests">Github 链接</a></p><h2 id="安装-1"><a href="#安装-1" class="headerlink" title="安装"></a>安装</h2><p><code>pip install requests -i https://pypi.tuna.tsinghua.edu.cn/simple</code></p><h2 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h2><p>一个类型和六个属性</p><ul><li>一个类型：<ul><li>Response 类型</li></ul></li><li>六个属性：<ul><li><code>r.text</code> : 获取网站源码</li><li><code>r.encoding</code> ：访问或定制编码方式（<code>r.encoding=r.apparent_encoding (自适应编码 / 万能编码)</code>）</li><li><code>r.url</code> ：获取请求的 url</li><li><code>r.content</code> ：响应的字节类型</li><li><code>r.status_code</code> ：响应的状态码</li><li><code>r.headers</code> ：响应的头信息</li></ul></li></ul><h2 id="请求类型"><a href="#请求类型" class="headerlink" title="请求类型"></a>请求类型</h2><h3 id="get请求"><a href="#get请求" class="headerlink" title="get请求"></a>get 请求</h3><ul><li>参数<strong>使用 params 传递</strong></li><li>参数<strong>无需 urlencode 编码</strong></li><li>不需要请求对象的定制</li><li>请求资源路径中的？可以加也可以不加</li></ul><h4 id="案例-1"><a href="#案例-1" class="headerlink" title="案例"></a>案例</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><br>url = <span class="hljs-string">'https://www.baidu.com/s'</span><br><br>headers = {<br>    <span class="hljs-string">'User-Agent'</span>: <span class="hljs-string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 '</span><br>                  <span class="hljs-string">'Safari/537.36'</span>,<br>    <span class="hljs-string">'Cookie'</span>: <span class="hljs-string">'xxx'</span><br>}<br><br>data = {<br>    <span class="hljs-string">'wd'</span>: <span class="hljs-string">'北京'</span><br>}<br><br><span class="hljs-comment"># url  请求资源路径</span><br><span class="hljs-comment"># params 参数</span><br><span class="hljs-comment"># kwargs 字典</span><br>response = requests.get(url=url, params=data, headers=headers)<br>response.encoding = <span class="hljs-string">'utf-8'</span><br>content = response.text<br><br><span class="hljs-built_in">print</span>(content)<br></code></pre></td></tr></tbody></table></figure><h3 id="post请求"><a href="#post请求" class="headerlink" title="post请求"></a>post 请求</h3><ul><li>post 请求是<strong>不需要编解码</strong></li><li>post 请求的<strong>参数是 data</strong></li><li>不需要请求对象的定制</li></ul><h4 id="案例-2"><a href="#案例-2" class="headerlink" title="案例"></a>案例</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> json<br><br><span class="hljs-keyword">import</span> requests<br><br>url = <span class="hljs-string">'https://fanyi.baidu.com/sug'</span><br><br>headers = {<br>    <span class="hljs-string">'User-Agent'</span>: <span class="hljs-string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'</span><br>}<br><br>data = {<br>    <span class="hljs-string">'kw'</span>: <span class="hljs-string">'reptile'</span><br>}<br><br><span class="hljs-comment"># url 请求地址</span><br><span class="hljs-comment"># data 请求参数</span><br><span class="hljs-comment"># kwargs 字典</span><br>response = requests.post(url=url, data=data, headers=headers)<br><br>content = response.text<br><br>obj = json.loads(content)<br><span class="hljs-built_in">print</span>(obj)<br></code></pre></td></tr></tbody></table></figure><h2 id="代理-1"><a href="#代理-1" class="headerlink" title="代理"></a>代理</h2><p>在 Python3.7 及以上版本，必须要在 <code>ip:port</code> 前面加上 <code>http://</code> 或者 <code>https://</code>，绝对不能去掉前面的 <code>http://</code> 或者 <code>https://</code>，即 Python3.7 后必须使用 <code>proxies={‘http’: ‘http://127.0.0.1:8080’}</code> 或者 <code>proxies={‘https’: ‘https://127.0.0.1:8080’}</code></p><h3 id="案例：使用requests代理"><a href="#案例：使用requests代理" class="headerlink" title="案例：使用requests代理"></a>案例：使用 requests 代理</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><br><span class="hljs-keyword">import</span> requests<br><br>url = <span class="hljs-string">'http://www.baidu.com/s?'</span><br><br>headers = {<br>    <span class="hljs-string">'Cookie'</span>: <span class="hljs-string">'xxx'</span>,<br>    <span class="hljs-string">'User-Agent'</span>: <span class="hljs-string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'</span>,<br>}<br><br>data = {<br>    <span class="hljs-string">'wd'</span>: <span class="hljs-string">'ip'</span><br>}<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_proxy</span>():<br>    proxy = requests.get(<span class="hljs-string">"http://demo.spiderpy.cn/get/"</span>).json().get(<span class="hljs-string">"proxy"</span>)<br>    <span class="hljs-keyword">return</span> {<span class="hljs-string">'http'</span>: <span class="hljs-string">'http://'</span> + proxy}<br><br><br>proxy = get_proxy()<br><span class="hljs-built_in">print</span>(proxy)<br>response = requests.get(url=url, params=data, headers=headers, proxies=proxy)<br><br>content = response.text<br><br><span class="hljs-comment"># 保存</span><br>filepath = <span class="hljs-string">'./download/'</span><br><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(filepath):<br>    os.makedirs(filepath)<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filepath + <span class="hljs-string">'proxy.html'</span>, <span class="hljs-string">'w'</span>, encoding=<span class="hljs-string">'utf-8'</span>) <span class="hljs-keyword">as</span> fp:<br>    fp.write(content)<br></code></pre></td></tr></tbody></table></figure><h2 id="处理验证码"><a href="#处理验证码" class="headerlink" title="处理验证码"></a>处理验证码</h2><ul><li>使用付费平台<a target="_blank" rel="noopener" href="https://www.chaojiying.com/">超级鹰</a></li><li>开源项目 <a target="_blank" rel="noopener" href="https://github.com/sml2h3/ddddocr">ddddocr</a>：<code>pip install ddddocr -i https://pypi.tuna.tsinghua.edu.cn/simple</code></li></ul><h2 id="案例：登录古诗文网"><a href="#案例：登录古诗文网" class="headerlink" title="案例：登录古诗文网"></a>案例：登录古诗文网</h2><p><strong>本案例使用 ddddocr 处理验证码</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 通过登陆  然后进入到主页面</span><br><br><span class="hljs-comment"># 登录接口的参数</span><br><span class="hljs-comment"># </span><br><span class="hljs-comment"># __VIEWSTATE: zqFkqiv+3ngBlBzmivZAdDumPi21G++2sOyB9IEEh6fdP620JtV5fRCeFk2oZjs7oL/nVneHKqG6GzJNAeDBNVOhTCVvk7Se5y3ahb8/m+eAyaOX2HCLv+bwmq4IN8ANi0N+Rwkzk5DDr2czD9C//EJmqSY=</span><br><span class="hljs-comment"># __VIEWSTATEGENERATOR: C93BE1AE</span><br><span class="hljs-comment"># from: http://so.gushiwen.cn/user/collect.aspx</span><br><span class="hljs-comment"># email: 18925681157</span><br><span class="hljs-comment"># pwd: test12</span><br><span class="hljs-comment"># code: iymh</span><br><span class="hljs-comment"># denglu: 登录</span><br><br><br><span class="hljs-comment"># 我们观察到__VIEWSTATE   __VIEWSTATEGENERATOR  code是一个可以变化的量</span><br><br><span class="hljs-comment"># 难点:(1)__VIEWSTATE   __VIEWSTATEGENERATOR  一般情况看不到的数据 都是在页面的源码中</span><br><span class="hljs-comment">#     我们观察到这两个数据在页面的源码中 所以我们需要获取页面的源码 然后进行解析就可以获取了</span><br><span class="hljs-comment">#     (2)验证码</span><br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-keyword">import</span> ddddocr<br><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">from</span> bs4 <span class="hljs-keyword">import</span> BeautifulSoup<br><br><span class="hljs-comment"># 登录页面的url</span><br>url = <span class="hljs-string">'https://so.gushiwen.cn/user/login.aspx?from=http://so.gushiwen.cn/user/collect.aspx'</span><br>headers = {<br>    <span class="hljs-string">'User-Agent'</span>: <span class="hljs-string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'</span><br>}<br><br>response = requests.get(url=url, headers=headers)<br>content = response.text<br><br><span class="hljs-comment"># 解析页面源码  然后获取__VIEWSTATE   __VIEWSTATEGENERATOR</span><br>soup = BeautifulSoup(content, <span class="hljs-string">'lxml'</span>)<br>__VIEWSTATE = soup.select(<span class="hljs-string">'#__VIEWSTATE'</span>)[<span class="hljs-number">0</span>].attrs.get(<span class="hljs-string">'value'</span>)<br>__VIEWSTATEGENERATOR = soup.select(<span class="hljs-string">'#__VIEWSTATEGENERATOR'</span>)[<span class="hljs-number">0</span>].attrs.get(<span class="hljs-string">'value'</span>)<br><br>email = <span class="hljs-built_in">input</span>(<span class="hljs-string">'请输入账号：'</span>)<br>pwd = <span class="hljs-built_in">input</span>(<span class="hljs-string">'请输入密码：'</span>)<br><span class="hljs-comment"># 获取验证码图片的url</span><br>code_url = <span class="hljs-string">'https://so.gushiwen.cn'</span> + soup.select(<span class="hljs-string">'#imgCode'</span>)[<span class="hljs-number">0</span>].attrs.get(<span class="hljs-string">'src'</span>)<br><span class="hljs-comment"># 获取了验证码的图片之后 下载到本地 然后观察验证码  观察之后 然后在控制台输入这个验证码 就可以将这个值给code的参数 就可以登陆</span><br><span class="hljs-comment"># 此处不能使用 urllib 等获取验证码的图片，原因是使用其会导致请求的两次验证码，获取的验证码不是登录时的验证码</span><br><span class="hljs-comment"># 应使用 session 对象里的验证码url</span><br><span class="hljs-comment"># 使用 .content 是因为图片需要用二进制打开</span><br>session = requests.session()<br>code_content = session.get(code_url).content<br>filepath = <span class="hljs-string">'./download/'</span><br><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(filepath):<br>    os.makedirs(filepath)<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filepath + <span class="hljs-string">'code.jpg'</span>, <span class="hljs-string">'wb'</span>) <span class="hljs-keyword">as</span> f:<br>    f.write(code_content)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">magic_ocr</span>(<span class="hljs-params">img_path</span>):<br>    <span class="hljs-string">"""</span><br><span class="hljs-string">    ocr 识别</span><br><span class="hljs-string">    :param img_path: 验证码图片路径</span><br><span class="hljs-string">    :return: </span><br><span class="hljs-string">    """</span><br>    ocr = ddddocr.DdddOcr()<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(img_path, <span class="hljs-string">'rb'</span>) <span class="hljs-keyword">as</span> f:<br>        img_bytes = f.read()<br>    <span class="hljs-keyword">return</span> ocr.classification(img_bytes)<br><br><br>code = magic_ocr(filepath + <span class="hljs-string">'code.jpg'</span>)<br><span class="hljs-built_in">print</span>(code)<br><br><span class="hljs-comment"># 点击登陆</span><br>url_post = <span class="hljs-string">'https://so.gushiwen.cn/user/login.aspx?from=http%3a%2f%2fso.gushiwen.cn%2fuser%2fcollect.aspx'</span><br><br>data_post = {<br>    <span class="hljs-string">'__VIEWSTATE'</span>: __VIEWSTATE,<br>    <span class="hljs-string">'__VIEWSTATEGENERATOR'</span>: __VIEWSTATEGENERATOR,<br>    <span class="hljs-string">'from'</span>: <span class="hljs-string">'http://so.gushiwen.cn/user/collect.aspx'</span>,<br>    <span class="hljs-string">'email'</span>: email,<br>    <span class="hljs-string">'pwd'</span>: pwd,<br>    <span class="hljs-string">'code'</span>: code,<br>    <span class="hljs-string">'denglu'</span>: <span class="hljs-string">'登录'</span>,<br>}<br><br><span class="hljs-comment"># 使用 session 对象登录</span><br>response_post = session.post(url=url, headers=headers, data=data_post)<br><br>content_post = response_post.text<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filepath + <span class="hljs-string">'古诗文网.html'</span>, <span class="hljs-string">'w'</span>, encoding=<span class="hljs-string">'utf-8'</span>) <span class="hljs-keyword">as</span> f:<br>    f.write(content_post)<br><br><span class="hljs-comment"># 难点</span><br><span class="hljs-comment"># （1） 隐藏域</span><br><span class="hljs-comment"># （2） 验证码</span><br></code></pre></td></tr></tbody></table></figure><p>运行结果：</p><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220818205428.png" srcset="/img/loading.gif" lazyload alt="image-20220818103916720"></p><h1 id="Scrapy"><a href="#Scrapy" class="headerlink" title="Scrapy"></a>Scrapy</h1><h2 id="Scrapy概念和工作流程"><a href="#Scrapy概念和工作流程" class="headerlink" title="Scrapy概念和工作流程"></a>Scrapy 概念和工作流程</h2><h3 id="简介-2"><a href="#简介-2" class="headerlink" title="简介"></a>简介</h3><p>scray 是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。</p><h3 id="安装-2"><a href="#安装-2" class="headerlink" title="安装"></a>安装</h3><p><code>pip install Scrapy -i https://pypi.tuna.tsinghua.edu.cn/simple</code></p><h3 id="Scrapy-工作原理"><a href="#Scrapy-工作原理" class="headerlink" title="Scrapy 工作原理"></a>Scrapy 工作原理</h3><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220819224546.png" srcset="/img/loading.gif" lazyload alt="20220819120154.png"></p><h3 id="Scrapy内置对象"><a href="#Scrapy内置对象" class="headerlink" title="Scrapy内置对象"></a>Scrapy 内置对象</h3><ul><li>request 请求对象：由 <code>url method post_data headers</code> 等构成</li><li>response 响应对象：由 <code>url body status headers</code> 等构成</li><li>item 数据对象：本质是个<strong>字典</strong></li></ul><h3 id="Scrapy各模块功能"><a href="#Scrapy各模块功能" class="headerlink" title="Scrapy各模块功能"></a>Scrapy 各模块功能</h3><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220819224913.png" srcset="/img/loading.gif" lazyload alt="20220819224913.png"></p><p><strong>注意：</strong>爬虫中间件和下载中间件只是运行逻辑的位置不同，作用是重复的：如替换 UA 等</p><h2 id="Scrapy入门使用"><a href="#Scrapy入门使用" class="headerlink" title="Scrapy入门使用"></a>Scrapy 入门使用</h2><ol><li><p>创建爬虫项目</p><p>在需要创建项目的目录，使用终端执行：<code>scrapy startproject 项目名字</code>，项目的名字<strong>不允许出现数字开头，也不能包含中文</strong>。</p></li><li><p>创建爬虫文件</p><p>需要在 spiders 文件夹中去创建爬虫文件。</p><p>进入 spiders 目录：<code>cd 项目名字/项目名字/spiders</code></p><p>创建爬虫文件：<code>scrapy genspider 爬虫文件名字 需要爬取的网页</code> (一般情况下不在网页前面加 <code>http/https</code>)</p><p>eg：<code>scrapy genspider baidu www.baidu.com</code></p></li><li><p>注释掉 <code>settings.py</code> 文件里的：<code>ROBOTSTXT_OBEY = True</code></p></li><li><p>运行爬虫文件</p><p><code>scrapy crawl 爬虫的名字</code></p><p>eg：<code>scrapy crawl baidu</code></p></li><li><p>提取数据<br>根据网站结构在 spider 中实现数据采集相关内容</p></li><li><p>保存数据<br>使用 pipeline 进行数据后续处理和保存</p></li></ol><h3 id="Scrapy文件目录"><a href="#Scrapy文件目录" class="headerlink" title="Scrapy文件目录"></a>Scrapy 文件目录</h3><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220819234618.png" srcset="/img/loading.gif" lazyload alt="20220819234618.png"></p><h3 id="response-的属性和方法"><a href="#response-的属性和方法" class="headerlink" title="response 的属性和方法"></a>response 的属性和方法</h3><ul><li><code>response.text</code> 获取的是响应的字符串</li><li><code>response.body</code> 获取的是二进制数据</li><li><code>response.xpath</code> 可以直接是 xpath 方法来解析 response 中的内容</li><li><code>response.extract()</code> 返回一个列表，提取 seletor 对象的 data 属性值</li><li><code>response.extract_first()</code> 返回一个字符串，提取的 seletor 列表的第一个数据</li></ul><h3 id="案例：当当网"><a href="#案例：当当网" class="headerlink" title="案例：当当网"></a>案例：当当网</h3><p>我们需要爬取当当网的书本图片链接，名字以及价格</p><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220819142219.png" srcset="/img/loading.gif" lazyload alt="20220819142219.png"></p><ol><li><p>定义数据结构为：要保存的 json 数据的对象</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Define here the models for your scraped items</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># See documentation in:</span><br><span class="hljs-comment"># https://docs.scrapy.org/en/latest/topics/items.html</span><br><br><span class="hljs-keyword">import</span> scrapy<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ScrapyDemo04DangdangItem</span>(scrapy.Item):<br>    <span class="hljs-comment"># define the fields for your item here like:</span><br>    <span class="hljs-comment"># name = scrapy.Field()</span><br><br>    <span class="hljs-comment"># 图片</span><br>    img_src = scrapy.Field()<br>    <span class="hljs-comment"># 名字</span><br>    name = scrapy.Field()<br>    <span class="hljs-comment"># 价格</span><br>    price = scrapy.Field()<br></code></pre></td></tr></tbody></table></figure></li><li><p>编写爬虫</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br><br><span class="hljs-keyword">from</span> scrapy_demo04_dangdang.items <span class="hljs-keyword">import</span> ScrapyDemo04DangdangItem<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DangdangSpider</span>(scrapy.Spider):<br>    name = <span class="hljs-string">'dangdang'</span><br>    allowed_domains = [<span class="hljs-string">'category.dangdang.com/cp01.47.93.00.00.00.html'</span>]<br>    start_urls = [<span class="hljs-string">'http://category.dangdang.com/cp01.47.93.00.00.00.html'</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">parse</span>(<span class="hljs-params">self, response</span>):<br>        <span class="hljs-comment"># pipelines 下载数据</span><br>        <span class="hljs-comment"># items    定义数据结构</span><br>        <span class="hljs-comment"># img_src = '//ul[@id="component_59"]/li//img/@data-original'</span><br>        <span class="hljs-comment"># name = '//ul[@id="component_59"]/li//img/@alt'</span><br>        <span class="hljs-comment"># price = '//ul[@id="component_59"]/li//p[@class="price"]/span[1]/text()'</span><br>        <span class="hljs-comment"># 所有的seletor对象都可以再次调用 xpath 方法</span><br>        li_list = response.xpath(<span class="hljs-string">'//ul[@id="component_59"]/li'</span>)<br>        <span class="hljs-keyword">for</span> li <span class="hljs-keyword">in</span> li_list:<br>            <span class="hljs-comment"># 第一张图片和其他的路径不一样，其他图片为懒加载</span><br>            img_src = li.xpath(<span class="hljs-string">'.//img/@data-original'</span>).extract_first()<br>            <span class="hljs-keyword">if</span> img_src:<br>                img_src = img_src<br>            <span class="hljs-keyword">else</span>:<br>                img_src = li.xpath(<span class="hljs-string">'.//img/@src'</span>).extract_first()<br>            name = li.xpath(<span class="hljs-string">'.//img/@alt'</span>).extract_first()<br>            price = li.xpath(<span class="hljs-string">'.//p[@class="price"]/span[1]/text()'</span>).extract_first()<br>            book = ScrapyDemo04DangdangItem(img_src=img_src, name=name, price=price)<br>            <span class="hljs-comment"># 获取一个book就将book返回给 pipelines</span><br>            <span class="hljs-keyword">yield</span> book<br></code></pre></td></tr></tbody></table></figure></li><li><p>重写写管道 pipeline</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Define your item pipelines here</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># Don't forget to add your pipeline to the ITEM_PIPELINES setting</span><br><span class="hljs-comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span><br><br><br><span class="hljs-comment"># useful for handling different item types with a single interface</span><br><br><span class="hljs-comment"># 如果想使用管道就要在 setting 中开启管道</span><br><span class="hljs-keyword">from</span> scrapy <span class="hljs-keyword">import</span> Request<br><span class="hljs-keyword">from</span> scrapy.exceptions <span class="hljs-keyword">import</span> DropItem<br><span class="hljs-keyword">from</span> scrapy.pipelines.images <span class="hljs-keyword">import</span> ImagesPipeline<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ScrapyDemo04DangdangPipeline</span>():<br>    <span class="hljs-comment"># 在爬虫文件开始前就执行</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">open_spider</span>(<span class="hljs-params">self, spider</span>):<br>        self.f = <span class="hljs-built_in">open</span>(<span class="hljs-string">'book.json'</span>, <span class="hljs-string">'w'</span>, encoding=<span class="hljs-string">'utf-8'</span>)<br><br>    <span class="hljs-comment"># item 就是 yeild 后的 book 对象</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">process_item</span>(<span class="hljs-params">self, item, spider</span>):<br>        self.f.write(<span class="hljs-built_in">str</span>(item))<br>        <span class="hljs-keyword">return</span> item<br><br>    <span class="hljs-comment"># 在爬虫文件结束后才执行</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">close_spider</span>(<span class="hljs-params">self, spider</span>):<br>        self.f.close()<br></code></pre></td></tr></tbody></table></figure></li><li><p>在 settings 中开启管道</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">ITEM_PIPELINES = {<br>    <span class="hljs-comment">#  管道是由很多个的，管道具有优先级，优先级范围为 1-1000 ，值越小优先级越高</span><br>    <span class="hljs-string">'scrapy_demo04_dangdang.pipelines.ScrapyDemo04DangdangPipeline'</span>: <span class="hljs-number">300</span>,<br>}<br></code></pre></td></tr></tbody></table></figure></li><li><p>运行 scrapy</p><p><code>scrapy crawl dangdang</code></p></li></ol><h2 id="Scrapy请求"><a href="#Scrapy请求" class="headerlink" title="Scrapy请求"></a>Scrapy 请求</h2><h3 id="翻页请求的思路"><a href="#翻页请求的思路" class="headerlink" title="翻页请求的思路"></a>翻页请求的思路</h3><p>对于下图所示的所有页面的数据该如何提取？</p><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220820012958.png" srcset="/img/loading.gif" lazyload alt="image-20220820012958392"></p><p>回顾 requests 模块是如何实现翻页请求的：</p><ol><li>找到下一页的 URL 地址</li><li>调用 requests.get (url)</li></ol><p>scrapy 实现翻页的思路：</p><ol><li>找到下一页的 url 地址</li><li>构造 url 地址的请求对象，传递给引擎</li></ol><h3 id="构造Request对象发送请求"><a href="#构造Request对象发送请求" class="headerlink" title="构造Request对象发送请求"></a>构造 Request 对象发送请求</h3><h4 id="实现方法"><a href="#实现方法" class="headerlink" title="实现方法"></a>实现方法</h4><ol><li>确定 url 地址</li><li>构造请求 <code>scrapy.Request(url,callback)</code><ul><li>callback：指定解析函数名称，表示该请求返回的响应使用哪一个函数进行解析</li></ul></li><li>把请求交给引擎：<code>yield scrapy.Request(url,callback)</code></li></ol><h4 id="模拟翻页"><a href="#模拟翻页" class="headerlink" title="模拟翻页"></a>模拟翻页</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br><br><span class="hljs-keyword">from</span> scrapy_demo04_dangdang.items <span class="hljs-keyword">import</span> ScrapyDemo04DangdangItem<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DangdangSpider</span>(scrapy.Spider):<br>    name = <span class="hljs-string">'dangdang'</span><br>    allowed_domains = [<span class="hljs-string">'category.dangdang.com'</span>]<br>    start_urls = [<span class="hljs-string">'http://category.dangdang.com/cp01.47.93.00.00.00.html'</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">parse</span>(<span class="hljs-params">self, response</span>):<br>        <span class="hljs-comment"># pipelines 下载数据</span><br>        <span class="hljs-comment"># items    定义数据结构</span><br>        <span class="hljs-comment"># img_src = '//ul[@id="component_59"]/li//img/@data-original'</span><br>        <span class="hljs-comment"># name = '//ul[@id="component_59"]/li//img/@alt'</span><br>        <span class="hljs-comment"># price = '//ul[@id="component_59"]/li//p[@class="price"]/span[1]/text()'</span><br>        <span class="hljs-comment"># 所有的seletor对象都可以再次调用 xpath 方法</span><br>        li_list = response.xpath(<span class="hljs-string">'//ul[@id="component_59"]/li'</span>)<br>        <span class="hljs-keyword">for</span> li <span class="hljs-keyword">in</span> li_list:<br>            <span class="hljs-comment"># 第一张图片和其他的路径不一样，其他图片为懒加载</span><br>            img_src = li.xpath(<span class="hljs-string">'.//img/@data-original'</span>).extract_first()<br>            <span class="hljs-keyword">if</span> img_src:<br>                img_src = img_src<br>            <span class="hljs-keyword">else</span>:<br>                img_src = li.xpath(<span class="hljs-string">'.//img/@src'</span>).extract_first()<br>            img_src = <span class="hljs-string">'http:'</span> + img_src<br>            name = li.xpath(<span class="hljs-string">'.//img/@alt'</span>).extract_first()<br>            price = li.xpath(<span class="hljs-string">'.//p[@class="price"]/span[1]/text()'</span>).extract_first()<br>            book = ScrapyDemo04DangdangItem(img_src=img_src, name=name, price=price)<br>            <span class="hljs-comment"># 获取一个book就将book返回给 pipelines</span><br>            <span class="hljs-keyword">yield</span> book<br>        <span class="hljs-comment"># 模拟翻页</span><br>        <span class="hljs-comment"># 获取翻页请求</span><br>        next_url = response.xpath(<span class="hljs-string">'//ul[@name="Fy"]/li[@class="next"]/a/@href'</span>).extract_first()<br>        <span class="hljs-comment"># 判断是否到最后一页</span><br>        <span class="hljs-keyword">if</span> next_url:<br>            <span class="hljs-comment"># 构造完整url</span><br>            url = response.urljoin(next_url)<br>            <span class="hljs-comment"># 构造scrapy.Request对象，并yield给引擎</span><br>            <span class="hljs-comment"># 利用callback参数指定该Request对象之后获取的响应用哪个函数进行解析</span><br>            <span class="hljs-keyword">yield</span> scrapy.Request(url, callback=self.parse)<br></code></pre></td></tr></tbody></table></figure><h3 id="scrapy-Request的更多参数"><a href="#scrapy-Request的更多参数" class="headerlink" title="scrapy.Request的更多参数"></a>scrapy.Request 的更多参数</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">scrapy.Request(url[,callback,method=<span class="hljs-string">"GET"</span>,headers,body,cookies,meta,dont_filter=<span class="hljs-literal">False</span>])<br></code></pre></td></tr></tbody></table></figure><h4 id="参数解释"><a href="#参数解释" class="headerlink" title="参数解释"></a>参数解释</h4><ol><li>中括号里的参数为可选参数</li><li><strong>callback</strong>：表示当前的 url 的响应交给哪个函数去处理</li><li><strong>meta</strong>：实现数据在不同的解析函数中传递，meta 默认带有部分数据，比如下载延迟，请求深度等</li><li>dont_filter: 默认为 False，会过滤请求的 url 地址，即请求过的 url 地址不会继续被请求，对需要重复请求的 url 地址可以把它设置为 Ture，比如贴吧的翻页请求，页面的数据总是在变化；start_urls 中的地址会被反复请求，否则程序不会启动</li><li>method：指定 POST 或 GET 请求</li><li>headers：接收一个字典，其中不包括 cookies</li><li>cookies：接收一个字典，专门放置 cookies</li><li>body：接收 json 字符串，为 POST 的数据，发送 payload_post 请求时使用（在下一章节中会介绍 post 请求）</li></ol><h3 id="meta参数的使用"><a href="#meta参数的使用" class="headerlink" title="meta参数的使用"></a>meta 参数的使用</h3><p><strong>meta 的作用</strong>：meta 可以实现数据在不同的解析函数中的传递</p><p>在爬虫文件的 parse 方法中，提取详情页增加之前 callback 指定的 parse_detail 函数：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">parse</span>(<span class="hljs-params">self,response</span>):<br>    ...<br>    <span class="hljs-keyword">yield</span> scrapy.Request(detail_url, callback=self.parse_detail,meta={<span class="hljs-string">"item"</span>:item})<br>...<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">parse_detail</span>(<span class="hljs-params">self,response</span>):<br>    <span class="hljs-comment">#获取之前传入的item</span><br>    item = resposne.meta[<span class="hljs-string">"item"</span>]<br></code></pre></td></tr></tbody></table></figure><h4 id="特别注意"><a href="#特别注意" class="headerlink" title="特别注意"></a>特别注意</h4><ol><li>meta 参数是一个字典</li><li>meta 字典中有一个固定的键 <code>proxy</code>，表示代理 ip，关于代理 ip 的使用我们将在 scrapy 的下载中间件的学习中进行介绍</li></ol><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><ol><li>完善并使用 Item 数据类：<ol><li>在 items.py 中完善要爬取的字段</li><li>在爬虫文件中先导入 Item</li><li>实例化 Item 对象后，像字典一样直接使用</li></ol></li><li>构造 Request 对象，并发送请求：<ol><li>导入 scrapy.Request 类</li><li>在解析函数中提取 url</li><li>yield scrapy.Request(url, callback=self.parse_detail, meta={})</li></ol></li><li>利用 meta 参数在不同的解析函数中传递数据:<ol><li>通过前一个解析函数 yield scrapy.Request (url, callback=self.xxx, meta={}) 来传递 meta</li><li>在 self.xxx 函数中 response.meta.get (‘key’, ‘’) 或 response.meta [‘key’] 的方式取出传递的数据</li></ol></li></ol><h2 id="Scrapy模拟登录"><a href="#Scrapy模拟登录" class="headerlink" title="Scrapy模拟登录"></a>Scrapy 模拟登录</h2><h3 id="requests-实现模拟登陆"><a href="#requests-实现模拟登陆" class="headerlink" title="requests 实现模拟登陆"></a>requests 实现模拟登陆</h3><ol><li>直接携带 cookies 请求页面</li><li>找 url 地址，发送 post 请求存储 cookie</li></ol><h3 id="selenium-模拟登陆"><a href="#selenium-模拟登陆" class="headerlink" title="selenium 模拟登陆"></a>selenium 模拟登陆</h3><p>找到对应的 input 标签，输入文本点击登陆</p><h3 id="scrapy-模拟登陆"><a href="#scrapy-模拟登陆" class="headerlink" title="scrapy 模拟登陆"></a>scrapy 模拟登陆</h3><ol><li>直接携带 cookies</li><li>找 url 地址，发送 post 请求存储 cookie</li></ol><h3 id="携带cookies-获取需要登陆的信息"><a href="#携带cookies-获取需要登陆的信息" class="headerlink" title="携带cookies 获取需要登陆的信息"></a>携带 cookies 获取需要登陆的信息</h3><p><strong>应用场景：</strong></p><ol><li>cookie 过期时间很长，常见于一些不规范的网站</li><li>能在 cookie 过期之前把所有的数据拿到</li><li>配合其他程序使用，比如其使用 selenium 把登陆之后的 cookie 获取到保存到本地，scrapy 发送请求之前先读取本地 cookie</li></ol><h4 id="重构starte-rquests方法"><a href="#重构starte-rquests方法" class="headerlink" title="重构starte_rquests方法"></a>重构 starte_rquests 方法</h4><p>scrapy 中 start_url 是通过 start_requests 来进行处理的，其实现代码如下</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 这是源代码</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">start_requests</span>(<span class="hljs-params">self</span>):<br>    cls = self.__class__<br>    <span class="hljs-keyword">if</span> method_is_overridden(cls, Spider, <span class="hljs-string">'make_requests_from_url'</span>):<br>        warnings.warn(<br>            <span class="hljs-string">"Spider.make_requests_from_url method is deprecated; it "</span><br>            <span class="hljs-string">"won't be called in future Scrapy releases. Please "</span><br>            <span class="hljs-string">"override Spider.start_requests method instead (see %s.%s)."</span> % (<br>                cls.__module__, cls.__name__<br>            ),<br>        )<br>        <span class="hljs-keyword">for</span> url <span class="hljs-keyword">in</span> self.start_urls:<br>            <span class="hljs-keyword">yield</span> self.make_requests_from_url(url)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">for</span> url <span class="hljs-keyword">in</span> self.start_urls:<br>            <span class="hljs-keyword">yield</span> Request(url, dont_filter=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></tbody></table></figure><p><strong>所以对应的，如果 start_url 地址中的 url 是需要登录后才能访问的 url 地址，则需要重写 start_request 方法并在其中手动添加上 cookie</strong></p><h4 id="携带cookies登陆github"><a href="#携带cookies登陆github" class="headerlink" title="携带cookies登陆github"></a>携带 cookies 登陆 github</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">GitSpider</span>(scrapy.Spider):<br>    name = <span class="hljs-string">'git1'</span><br>    allowed_domains = [<span class="hljs-string">'github.com'</span>]<br>    start_urls = [<span class="hljs-string">'https://github.com/AWeiIsCoding'</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">start_requests</span>(<span class="hljs-params">self</span>):<br>        url = self.start_urls[<span class="hljs-number">0</span>]<br>        cookie_temp = <span class="hljs-string">'从网站上拿到的cookie'</span><br>        cookies = {data.split(<span class="hljs-string">'='</span>)[<span class="hljs-number">0</span>]: data.split(<span class="hljs-string">'='</span>)[-<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> cookie_temp.split(<span class="hljs-string">'; '</span>)}<br>        <span class="hljs-keyword">yield</span> scrapy.Request(url, callback=self.parse, cookies=cookies)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">parse</span>(<span class="hljs-params">self, response</span>):<br>        <span class="hljs-built_in">print</span>(response.xpath(<span class="hljs-string">'/html/head/title/text()'</span>).extract_first())<br></code></pre></td></tr></tbody></table></figure><p>登录成功之后，输出的标题后不会带上<code>· GitHub</code></p><p>登录成功：</p><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220823013521.png" srcset="/img/loading.gif" lazyload alt="image-20220823013521495"></p><p>登录失败：</p><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220823013603.png" srcset="/img/loading.gif" lazyload alt="image-20220823013603745"></p><p><strong>注意：</strong></p><ol><li>scrapy 中 cookie 不能够放在 headers 中，在构造请求的时候有专门的 cookies 参数，能够接受字典形式的 coookie</li><li>在 setting 中设置 ROBOTS 协议、USER_AGENT</li></ol><h3 id="scrapy-发送post请求"><a href="#scrapy-发送post请求" class="headerlink" title="scrapy 发送post请求"></a>scrapy 发送 post 请求</h3><p>我们知道可以通过 scrapy.Request () 指定 method、body 参数来发送 post 请求；<strong>但是通常使用 scrapy.FormRequest () 来发送 post 请求。</strong></p><p><strong>注意：scrapy.FormRequest () 能够发送表单和 ajax 请求</strong></p><h4 id="思路分析"><a href="#思路分析" class="headerlink" title="思路分析"></a>思路分析</h4><ol><li>找到 post 的 url 地址：点击登录按钮进行抓包，然后定位 url 地址为 <a target="_blank" rel="noopener" href="https://github.com/session">https://github.com/session</a></li><li>找到请求体的规律：分析 post 请求的请求体，其中包含的参数均在前一次的响应中</li><li>否登录成功：通过请求个人主页，观察是否包含用户名</li></ol><h4 id="案例：登录Github获取首页"><a href="#案例：登录Github获取首页" class="headerlink" title="案例：登录Github获取首页"></a>案例：登录 Github 获取首页</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Git2Spider</span>(scrapy.Spider):<br>    name = <span class="hljs-string">'git2'</span><br>    allowed_domains = [<span class="hljs-string">'github.com'</span>]<br>    start_urls = [<span class="hljs-string">'https://github.com/login'</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">parse</span>(<span class="hljs-params">self, response</span>):<br>        <span class="hljs-comment"># 从登录页面响应中解析出post数据</span><br>        authenticity_token = response.xpath(<span class="hljs-string">"//input[@name='authenticity_token']/@value"</span>).extract_first()<br>        utf8 = response.xpath(<span class="hljs-string">"//input[@name='utf8']/@value"</span>).extract_first()<br>        commit = response.xpath(<span class="hljs-string">"//input[@name='commit']/@value"</span>).extract_first()<br>        webauthn_support = response.xpath(<span class="hljs-string">"//input[@name='webauthn-support']/@value"</span>).extract_first()<br>        webauthn_iuvpaa_support = response.xpath(<span class="hljs-string">"//input[@name='webauthn-iuvpaa-support']/@value"</span>).extract_first()<br>        return_to = response.xpath(<span class="hljs-string">"//input[@name='return_to']/@value"</span>).extract_first()<br>        timestamp = response.xpath(<span class="hljs-string">"//input[@name='timestamp']/@value"</span>).extract_first()<br>        timestamp_secret = response.xpath(<span class="hljs-string">"//input[@name='timestamp_secret']/@value"</span>).extract_first()<br>        post_data = {<br>            <span class="hljs-string">'commit'</span>: commit,<br>            <span class="hljs-string">'authenticity_token'</span>: authenticity_token,<br>            <span class="hljs-string">'login'</span>: <span class="hljs-string">'账号'</span>,<br>            <span class="hljs-string">'password'</span>: <span class="hljs-string">'密码'</span>,<br>            <span class="hljs-string">'trusted_device'</span>: <span class="hljs-string">''</span>,<br>            <span class="hljs-string">'webauthn-support'</span>: webauthn_support,<br>            <span class="hljs-string">'webauthn-iuvpaa-support'</span>: webauthn_iuvpaa_support,<br>            <span class="hljs-string">'return_to'</span>: return_to,<br>            <span class="hljs-string">'allow_signup'</span>: <span class="hljs-string">''</span>,<br>            <span class="hljs-string">'client_id'</span>: <span class="hljs-string">''</span>,<br>            <span class="hljs-string">'integration'</span>: <span class="hljs-string">''</span>,<br>            <span class="hljs-string">'required_field_f5d1'</span>: <span class="hljs-string">''</span>,<br>            <span class="hljs-string">'timestamp'</span>: timestamp,<br>            <span class="hljs-string">'timestamp_secret'</span>: timestamp_secret,<br>        }<br>        <span class="hljs-built_in">print</span>(post_data)<br>        <span class="hljs-comment"># 针对登陆的url发送post请求</span><br>        <span class="hljs-keyword">yield</span> scrapy.FormRequest(url=<span class="hljs-string">'https://github.com/session'</span>, formdata=post_data, callback=self.after_login)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">after_login</span>(<span class="hljs-params">self, response</span>):<br>        <span class="hljs-keyword">yield</span> scrapy.FormRequest(url=<span class="hljs-string">'https://github.com/1337720537'</span>, callback=self.cheak_login)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">cheak_login</span>(<span class="hljs-params">self, response</span>):<br>        <span class="hljs-built_in">print</span>(response.xpath(<span class="hljs-string">'/html/head/title/text()'</span>).extract_first())<br></code></pre></td></tr></tbody></table></figure><p>运行结果：</p><p><img src="https://inencoding.oss-cn-shenzhen.aliyuncs.com/img/20220824011021.png" srcset="/img/loading.gif" lazyload alt="image-20220824011021609"></p><p>在 settings.py 中通过设置 <code>COOKIES_DEBUG=TRUE</code> 能够在终端看到 cookie 的传递传递过程</p><h2 id="Scrapy-Shell"><a href="#Scrapy-Shell" class="headerlink" title="Scrapy Shell"></a>Scrapy Shell</h2><p><strong>Scrapy Shell，是一个交互终端</strong>，供您<strong>在未启动 spider 的情况下尝试及调试您的爬取代码</strong>。 其本意是用来测试提取数据的代码，不过您可以将其作为正常的 Python 终端，在上面测试任何的 Python 代码。 该终端是用来测试 XPath 或 CSS 表达式，查看他们的工作方式及从爬取的网页中提取的数据。 在编写您的 spider 时，该终端提供了交互性测试您的表达式代码的功能，<strong>免去了每次修改后运行 spider 的麻烦</strong>。 一旦熟悉了 Scrapy 终端后，您会发现其在开发和调试 spider 时发挥的巨大作用。</p><ul><li><p>安装</p><p><code>pip install ipython -i https://pypi.tuna.tsinghua.edu.cn/simple</code></p></li><li><p>如何使用</p><ul><li>如果需要进入 scrapy shell ，直接在终端中输入 <code>scrapy shell 域名</code>就行了。</li><li>如果想要高亮或者自动补全，就需要安装 ipython 。</li></ul></li></ul><h2 id="Scrapy管道"><a href="#Scrapy管道" class="headerlink" title="Scrapy管道"></a>Scrapy 管道</h2><h3 id="pipeline-常用的方法"><a href="#pipeline-常用的方法" class="headerlink" title="pipeline 常用的方法"></a>pipeline 常用的方法</h3><ol><li>process_item(self,item,spider):<ul><li>管道类中必须有的函数</li><li>实现对 item 数据的处理</li><li>必须 return item</li></ul></li><li>open_spider (self, spider): 在爬虫开启的时候仅执行一次</li><li>close_spider (self, spider): 在爬虫关闭的时候仅执行一次</li></ol></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="category-chain-item">学习笔记</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/%E7%88%AC%E8%99%AB/">#爬虫</a></div></div><div class="license-box my-3"><div class="license-title"><div>python 爬虫</div><div>https://www.inencoding.com/posts/62461/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>AWEI</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2022年8月13日</div></div><div class="license-meta-item license-meta-date"><div>更新于</div><div>2022年12月8日</div></div><div class="license-meta-item"><div>许可协议</div><div><a target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a><a target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="NC - 非商业性使用"><i class="iconfont icon-nc"></i></span></a><a target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="SA - 相同方式共享"><i class="iconfont icon-sa"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/posts/14402/" title="Docker 常用环境搭建"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">Docker 常用环境搭建</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/posts/40991/" title="Docker"><span class="hidden-mobile">Docker</span> <span class="visible-mobile">下一篇</span><i class="iconfont icon-arrowright"></i></a></article></div></div></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a><i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a><div><span id="timeDate">载入天数...</span> <span id="times">载入时分秒...</span><script src="/js/duration.js"></script></div></div><div class="statistics"><span id="busuanzi_container_site_pv" style="display:none">总访问量<span id="busuanzi_value_site_pv"></span> 次</span> <span id="busuanzi_container_site_uv" style="display:none">总访客数<span id="busuanzi_value_site_uv"></span> 人</span></div><div class="beian"><span><a href="http://beian.miit.gov.cn/" target="_blank" rel="nofollow noopener">粤ICP备2021025468号</a></span></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t){var e=Fluid.plugins.typing;(t=t.getElementById("subtitle"))&&e&&e(t.getAttribute("data-typed-text"))}((window,document))</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js",(function(){var t,o=jQuery("#toc");0!==o.length&&window.tocbot&&(t=jQuery("#board-ctn").offset().top,window.tocbot.init({tocSelector:"#toc-body",contentSelector:".markdown-body",headingSelector:CONFIG.toc.headingSelector||"h1,h2,h3,h4,h5,h6",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",collapseDepth:CONFIG.toc.collapseDepth||0,scrollSmooth:!0,headingsOffset:-t}),0<o.find(".toc-list-item").length&&o.css("visibility","visible"))}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var o,n=[];for(o of(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","))n.push(".markdown-body > "+o.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(n.join(", "))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script src="/js/local-search.js"></script><script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript><script src="https://fastly.jsdelivr.net/npm/hexo-tag-common@0.1.0/js/index.js"></script><script async>window.onload=function(){var e=document.createElement("script"),t=document.getElementsByTagName("script")[0];e.type="text/javascript",e.async=!0,e.src="/sw-register.js?v="+Date.now(),t.parentNode.insertBefore(e,t)}</script></body></html>